# âœ… ìµœì‹  ë¡œì§ í†µí•©ëœ Streamlit UI ì½”ë“œ (í”„ë¡¬í”„íŠ¸ í¬í•¨ ì¬ì •ë¹„)

import streamlit as st
import faiss
import pickle
import numpy as np
import random
import itertools
from collections import deque
from openai import OpenAI
from datetime import datetime
from zoneinfo import ZoneInfo

# âœ… ì „ì—­ ìƒìˆ˜
SIMILARITY_THRESHOLD = 0.30
MAX_HISTORY_LEN = 5

# âœ… ì‚¬ìš©ì ì§ˆë¬¸ ì „ì²´ íˆìŠ¤í† ë¦¬ ì €ì¥ ë¦¬ìŠ¤íŠ¸ (ë¬´í•œ ì €ì¥)
user_query_history = []

# âœ… ìƒíƒœ ì´ˆê¸°í™”
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = [{
        "role": "system",
        "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ ì•„ë˜ ì§€ì¹¨ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:\n\n- ë‹µë³€ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ ë§íˆ¬ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n- ì‚¬ìš©ì ì§ˆë¬¸ì— \"ì¶”ì²œ\", \"ì œì‹œ\", \"ì°¾ì•„\", \"ê²€ìƒ‰í•´\" ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ë©´ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ê° í•­ëª©ì— ê¸°ì—…ëª…, ì„œë¹„ìŠ¤ëª…, ê¸°ì—…ID, ì„œë¹„ìŠ¤ìœ í˜•, ê¸ˆì•¡, ê¸°í•œ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.\n- ëª©ë¡ì€ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:\n  1. \"ì„œë¹„ìŠ¤ëª…\"\n     - \"ê¸°ì—…ëª…\" (ê¸°ì—…ID: XXXX)\n     - ìœ í˜•: ...\n     - ê¸ˆì•¡: ...\n     - ê¸°í•œ: ...\n     - ìš”ì•½: ...\n- íŠ¹ìˆ˜ë¬¸ì(**, ## ë“±)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , ë¶ˆë¦¿ì€ ëŒ€ì‹œ(-)ë¡œë§Œ í†µì¼í•´ ì£¼ì„¸ìš”. í•­ëª© ê°„ ê°œí–‰ ì—†ì´ ì´ì–´ì„œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n- ê¸°ì—…ëª…/ì„œë¹„ìŠ¤ëª…ì´ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ ì´ìœ ë„ í•¨ê»˜ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n- ê¸°ì—…IDëŠ” ë°˜ë“œì‹œ ê´„í˜¸ ì•ˆì— í‘œê¸°í•´ ì£¼ì„¸ìš”. ì˜ˆ: \"ì œì´ì–´ìŠ¤\" (ê¸°ì—…ID: 12345)"
    }]
if "all_results" not in st.session_state:
    st.session_state.all_results = deque(maxlen=MAX_HISTORY_LEN)
if "excluded_keys" not in st.session_state:
    st.session_state.excluded_keys = set()
if "last_results" not in st.session_state:
    st.session_state.last_results = []
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []
if "embedding_cache" not in st.session_state:
    st.session_state.embedding_cache = {}

# âœ… GPT ë° FAISS ì„¸íŒ…
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
index = faiss.read_index("service_index.faiss")
with open("service_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

xb = index.reconstruct_n(0, index.ntotal)
xb = xb / np.linalg.norm(xb, axis=1, keepdims=True)
d = xb.shape[1]
index_cosine = faiss.IndexFlatIP(d)
index_cosine.add(xb)

# âœ… í•¨ìˆ˜ ì •ì˜

# âœ… ë³´ì™„ëœ ìƒíƒœ ë³€ìˆ˜
if "user_query_history" not in st.session_state:
    st.session_state.user_query_history = deque(maxlen=5)
if "embedding_query_text" not in st.session_state:
    st.session_state.embedding_query_text = None

# âœ… GPT í›„ì† ì§ˆë¬¸ íŒë‹¨ ìºì‹œ ì €ì¥ì†Œ (ì „ì—­)
followup_cache = {}

# âœ… í›„ì† ì§ˆë¬¸ ì—¬ë¶€ íŒë‹¨
def is_followup_question(prev, current):
    key = (prev.strip(), current.strip())  # ì „ì²˜ë¦¬ëœ ì§ˆë¬¸ ìŒì„ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©

    if key in followup_cache:
        st.writer(f"âš ï¸ [CACHE HIT] Cacheì— í›„ì† ì§ˆë¬¸ ì—¬ë¶€ íŒë‹¨ ì™„ë£Œ: {key}")
        return followup_cache[key]

    st.writer(f"ğŸ§  [CACHE MISS] ChatGPTì— í›„ì† ì§ˆë¬¸ ì—¬ë¶€ íŒë‹¨ ì¤‘: {key}")
    messages = [
        {"role": "system", "content": "ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨í•´ ì£¼ì„¸ìš”. í›„ì†ì´ë©´ YES, ì•„ë‹ˆë©´ NOë¡œë§Œ ë‹µí•´ ì£¼ì„¸ìš”."},
        {"role": "user", "content": f"ì´ì „ ì§ˆë¬¸: {prev}\ní˜„ì¬ ì§ˆë¬¸: {current}"}
    ]
    try:
        reply = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=messages
        )
        answer = reply['choices'][0]['message']['content'].strip().lower()
        result = "yes" in answer  # 'yes' í¬í•¨ ì—¬ë¶€ë¡œ íŒë‹¨
        followup_cache[key] = result  # âœ… ìºì‹œ ì €ì¥
        return result
    except Exception as e:
        st.writer(f"[âŒ GPT ì˜¤ë¥˜] í›„ì† ì§ˆë¬¸ íŒë‹¨ ì‹¤íŒ¨: {e}")
        return True  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì€ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼

# âœ… ê²°ê³¼ê°€ ì¶©ë¶„í•œì§€ íŒë‹¨
def is_related_results_enough(results):
    st.writer("âš ï¸ [INFO] ì¶”ì²œ ê²°ê³¼ì˜ ì—°ê´€ì„±ì´ ë‚®ì•„ GPT í˜¸ì¶œì„ ìƒëµí•©ë‹ˆë‹¤.")
    return results and len(results) >= 3

# âœ… GPT ì‘ë‹µì—ì„œ ì‹¤ì œ ì–¸ê¸‰ëœ í‚¤ë§Œ ì¶”ì¶œ
def parse_referenced_keys(response_text, result_list):
    referenced = set()
    for s in result_list:
        if str(s["ê¸°ì—…ID"]) in response_text and s["ì„œë¹„ìŠ¤ëª…"] in response_text:
            referenced.add((s["ê¸°ì—…ID"], s.get("ì„œë¹„ìŠ¤ìœ í˜•"), s.get("ì„œë¹„ìŠ¤ëª…")))
    return referenced

def get_kst_time():
    return datetime.now(ZoneInfo("Asia/Seoul")).strftime("%p %I:%M")

def get_embedding(text, model="text-embedding-3-small"):
    if text in st.session_state.embedding_cache:
        return st.session_state.embedding_cache[text]
    response = client.embeddings.create(input=[text], model=model)
    embedding = response.data[0].embedding
    st.session_state.embedding_cache[text] = embedding
    return embedding

def ask_gpt(messages):
    response = client.chat.completions.create(model="gpt-4o", messages=messages)
    return response.choices[0].message.content

def is_best_recommendation_query(query):
    return any(k in query for k in ["ê°•ë ¥ ì¶”ì²œ", "ê°•ì¶”"])

def is_relevant_question(query, threshold=SIMILARITY_THRESHOLD):
    vec = np.array(get_embedding(query)).astype('float32').reshape(1, -1)
    vec /= np.linalg.norm(vec, axis=1, keepdims=True)
    D, _ = index_cosine.search(vec, 1)
    return D[0][0] >= threshold

def recommend_services(query, top_k=5, exclude_keys=None, use_random=True):
    if exclude_keys is None:
        exclude_keys = set()
    vec = np.array(get_embedding(query)).astype('float32').reshape(1, -1)
    vec /= np.linalg.norm(vec, axis=1, keepdims=True)
    D, indices = index_cosine.search(vec, 300)

    ranked = [(score, metadata[idx]) for score, idx in zip(D[0], indices[0])]
    
    # ğŸ“Œ STEP 1: ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ëœ ì›ë³¸ ìƒìœ„ 30ê°œ ì¶œë ¥
    st.write(f"\nğŸ“Œ [STEP 1] ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ëœ ì›ë³¸ ìƒìœ„ 30ê°œ:")
    for i, (score, s) in enumerate(ranked[:30]):
        st.write(f"{i+1}. [{score:.4f}] {s['ê¸°ì—…ëª…']} / {s.get('ì„œë¹„ìŠ¤ìœ í˜•')} / {s.get('ì„œë¹„ìŠ¤ëª…')}")

    # âœ… 4. ì œì™¸í•  í‚¤ (ê¸°ì—…ID + ì„œë¹„ìŠ¤ìœ í˜• + ì„œë¹„ìŠ¤ëª…) ì •ì˜
    if exclude_keys:
        st.write(f"\nğŸš« [STEP 2] ì œì™¸ ëŒ€ìƒ í‚¤ ìˆ˜: {len(exclude_keys)}")
        for i, key in enumerate(list(exclude_keys)[:10]):
            st.write(f" - ì œì™¸ {i+1}: ê¸°ì—…ID={key[0]} / {key[1]} / {key[2]}")
    else:
        st.write("\nğŸš« [STEP 2] ì œì™¸ ëŒ€ìƒ ì—†ìŒ")

    seen_keys = set()
    filtered = []
    for score, s in ranked:
        key = (s["ê¸°ì—…ID"], s.get("ì„œë¹„ìŠ¤ìœ í˜•"), s.get("ì„œë¹„ìŠ¤ëª…"))
        if key in exclude_keys or key in seen_keys:
            continue
        seen_keys.add(key)
        filtered.append((score, s))

    filtered.sort(key=lambda x: x[0], reverse=True)
    
    # âœ… ìƒìœ„ 30ê°œê¹Œì§€ ì¶œë ¥ (ë””ë²„ê¹… ë˜ëŠ” ë¡œê·¸ í™•ì¸ìš©)
    st.write(f"\nâœ… [STEP 3] í•„í„°ë§ í›„ ìƒìœ„ 30ê°œ:")
    for i, (score, s) in enumerate(filtered[:30]):
        st.write(f"{i+1}. [{score:.4f}] {s['ê¸°ì—…ëª…']} / {s.get('ì„œë¹„ìŠ¤ìœ í˜•')} / {s.get('ì„œë¹„ìŠ¤ëª…')}")

    if use_random:
        top_10 = filtered[:10]
        selected = random.sample(top_10, min(len(top_10), top_k))
        return [s for _, s in selected]
    return [s for _, s in filtered[:top_k]]

def make_context(results):
    return "\n".join([
        f"{i+1}. \"{s['ì„œë¹„ìŠ¤ëª…']}\" (ì œê³µê¸°ì—…: \"{s['ê¸°ì—…ëª…']}\", ê¸°ì—…ID: {s['ê¸°ì—…ID']})\n"
        f"- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}\n"
        f"- ê¸ˆì•¡: {s.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')} / ê¸°í•œ: {s.get('ì„œë¹„ìŠ¤ê¸°í•œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ë²•ì¸ì—¬ë¶€: {s.get('ê¸°ì—…ì˜ ë²•ì¸ì—¬ë¶€', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìœ„ì¹˜: {s.get('ê¸°ì—… ìœ„ì¹˜', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•µì‹¬ì—­ëŸ‰: {s.get('ê¸°ì—… í•µì‹¬ì—­ëŸ‰', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- 3ê°œë…„ í‰ê·  ë§¤ì¶œ: {s.get('ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•´ë‹¹ë¶„ì•¼ì—…ë ¥: {s.get('ê¸°ì—… í•´ë‹¹ë¶„ì•¼ì—…ë ¥', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì£¼ìš”ì‚¬ì—…ë‚´ìš©: {s.get('ê¸°ì—… ì£¼ìš”ì‚¬ì—…ë‚´ìš©', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì¸ë ¥í˜„í™©: {s.get('ê¸°ì—… ì¸ë ¥í˜„í™©', 'ì •ë³´ ì—†ìŒ')}"
        for i, s in enumerate(results)
    ])

def make_summary_context(summary_memory):
    seen = set()
    deduped = []
    for result_list in reversed(summary_memory):
        for item in result_list:
            key = (item['ì„œë¹„ìŠ¤ëª…'], item['ê¸°ì—…ëª…'], item.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì—†ìŒ'))
            if key not in seen:
                seen.add(key)
                deduped.insert(0, item)
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}"
        for i, s in enumerate(deduped)
    ])

def make_prompt(query, context, is_best=False):
    recent_queries = "\n".join(f"- {q}" for q in st.session_state.user_query_history)
    style_instruction = (
        "- ë‹µë³€ì€ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”. ê° ì¶”ì²œ í•­ëª©ì€ ë²ˆí˜¸ë¥¼ ë¶™ì´ê³ ,"
        "ê¸°ì—…ëª…, ì„œë¹„ìŠ¤ëª…, ì„œë¹„ìŠ¤ ìœ í˜•, ê¸ˆì•¡, ê¸°í•œ, ë²•ì¸ì—¬ë¶€, ìœ„ì¹˜, í•µì‹¬ì—­ëŸ‰, 3ê°œë…„ í‰ê·  ë§¤ì¶œ, í•´ë‹¹ë¶„ì•¼ì—…ë ¥, ì£¼ìš”ì‚¬ì—…ë‚´ìš©, ì¸ë ¥í˜„í™©ì„ ìƒì„¸í•˜ê²Œ ê¸°ìˆ í•´ ì£¼ì„¸ìš”.\n"
        "- ë‹µë³€ ì‹œ ë°˜ë“œì‹œ ì„œë¹„ìŠ¤ëª…ê³¼ ê¸°ì—…ëª…ì€ ë”°ì˜´í‘œë¡œ ë¬¶ì–´ì£¼ì‹œê³ , ëª©ë¡ í‘œê¸° ì‹œì—ëŠ” ëŒ€ì‹œ(-)ë¡œë§Œ ë‚˜ì—´í•´ ì£¼ì„¸ìš”.\n"
        "- ë§Œì•½ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì¶©ë¶„í•œ ì—°ê´€ì„±ì´ ì—†ë‹¤ê³  íŒë‹¨ë˜ë©´, ì§ì ‘ì ì¸ ì¶”ì²œì´ ì–´ë µë‹¤ê³  ë¨¼ì € ë§ì”€í•´ ì£¼ì„¸ìš”.\n"
        "- ë‹¨, ìœ ì‚¬í•œ í‚¤ì›Œë“œë‚˜ ì°¸ê³ ê°€ ë  ë§Œí•œ ì„œë¹„ìŠ¤ê°€ ìˆë‹¤ë©´ ìµœëŒ€ 1~2ê°œë§Œ ì˜ˆì‹œë¡œ ì†Œê°œí•´ ì£¼ì„¸ìš”.\n"
        "- ë¶ˆí•„ìš”í•œ íŠ¹ìˆ˜ë¬¸ì(**, ## ë“±)ì€ ì‚¬ìš©í•˜ì§€ ë§ì•„ ì£¼ì„¸ìš”.\n"
        "- ê° ì¶”ì²œ í•­ëª© ì„¤ëª… ì‹œ ë°˜ë“œì‹œ ê¸°ì—…IDë¥¼ ê´„í˜¸ ì•ˆì— ë°˜ë“œì‹œ í˜•íƒœë¡œ ëª…ì‹œí•´ ì£¼ì„¸ìš”. ì˜ˆ: \"ê¸°ì—…ëª…\" (ê¸°ì—…ID: 1234)"
    )
    extra = f"ì§€ê¸ˆê¹Œì§€ ì¶”ì²œí•œ ì„œë¹„ìŠ¤ ëª©ë¡:\n\n{make_summary_context(st.session_state.all_results)}\n\nì´ì „ì— ì¶”ì²œëœ ê¸°ì—…ë„ í¬í•¨í•´ì„œ ì¡°ê±´ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ìµœê³ ì˜ ì¡°í•©ì„ ì œì‹œí•´ì£¼ì„¸ìš”." if is_best else ""

    return f"""
ë‹¹ì‹ ì€ ê´€ê´‘ìˆ˜í˜œê¸°ì—…ì—ê²Œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” AI ìƒë‹´ì‚¬ í˜¸ì¢…ì´ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
"{query}"

ê´€ë ¨ëœ ì„œë¹„ìŠ¤ ëª©ë¡:
{context}

ğŸ“Œ {extra}
ğŸ“Œ ìµœê·¼ ì‚¬ìš©ì ì§ˆë¬¸ ì´ë ¥:\n" + recent_queries + "\n\nğŸ“Œ ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œì„œ ë‹µë³€í•´ì£¼ì„¸ìš”:
{style_instruction}
"""

# âœ… UI ë Œë”ë§ ë° ì…ë ¥ ì²˜ë¦¬
st.title("í˜ì‹ ë°”ìš°ì²˜ ì„œë¹„ìŠ¤ íŒŒì¸ë”")
st.write("ğŸ¤– í˜¸ì¢…ì´ì—ê²Œ ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.")

for msg in st.session_state.chat_messages:
    role_class = "user-msg" if msg["role"] == "user" else "chatbot-msg"
    time_class = "user-msg-time" if msg["role"] == "user" else "chatbot-msg-time"
    box_class = "user-msg-box" if msg["role"] == "user" else "chatbot-msg-box"
    st.markdown(f"""
    <div class='{box_class}'>
        <div class='{role_class}'>
            {msg['content'].replace(chr(10), '<br>')}
            <div class='{time_class}'>{msg['timestamp']}</div>
        </div>
    </div>
    """, unsafe_allow_html=True)

with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_area("", height=100, label_visibility="collapsed")
    submitted = st.form_submit_button("ë¬¼ì–´ë³´ê¸°")

if submitted and user_input.strip():
    if user_input == st.session_state.get("embedding_query_text"):
        reply = "âš ï¸ ë™ì¼í•œ ì§ˆë¬¸ì´ ë°˜ë³µë˜ì–´ GPT ì‘ë‹µì„ ìƒëµí•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°”ê¿” ì£¼ì„¸ìš”."
        st.session_state.chat_messages.append({"role": "assistant", "content": reply, "timestamp": get_kst_time()})
        st.rerun()

    st.session_state.embedding_query_text = user_input
    st.session_state.user_query_history.append(user_input)

    st.session_state.chat_messages.append({"role": "user", "content": user_input, "timestamp": get_kst_time()})

    if user_input.startswith("ìì„¸íˆ"):
        keyword = user_input.replace("ìì„¸íˆ", "").strip()
        all_results = list(itertools.chain.from_iterable(st.session_state.all_results))
        matches = [s for s in all_results if keyword in s["ê¸°ì—…ëª…"]]

        if not matches:
            reply = "â— í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê¸°ì—…ëª…ì´ ì—†ìŠµë‹ˆë‹¤."
        elif len(matches) > 1:
            reply = "â— ì—¬ëŸ¬ ê°œì˜ ê¸°ì—…ëª…ì´ ì¼ì¹˜í•©ë‹ˆë‹¤:<br>" + "<br>".join(f"- {s['ê¸°ì—…ëª…']}" for s in matches)
        else:
            s = matches[0]
            details = []
            for k, v in s.items():
                if k == "ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ":
                    try:
                        num = float(v)
                        v = f"{int(num):,}ì›"
                    except:
                        pass
                elif k == "ê¸°ì—… ì¸ë ¥í˜„í™©":
                    try:
                        num = float(v)
                        v = f"{int(num)}ëª…"
                    except:
                        pass
                elif k == "ê¸°ì—… í•µì‹¬ì—­ëŸ‰":
                    try:
                        v = v.replace("_x000D_", "")
                    except:
                        pass
                details.append(f"â€¢ {k}: {v}")

            service_link = f"https://www.tourvoucher.or.kr/user/svcManage/svc/BD_selectSvc.do?svcNo={s['ì„œë¹„ìŠ¤ë²ˆí˜¸']}"
            company_link = f"https://www.tourvoucher.or.kr/user/entrprsManage/provdEntrprs/BD_selectProvdEntrprs.do?entrprsId={s['ê¸°ì—…ID']}"
            details.append(f"ğŸ”— <b>ì„œë¹„ìŠ¤ ë§í¬:</b> <a href='{service_link}' target='_blank'>{service_link}</a>")
            details.append(f"ğŸ¢ <b>ê¸°ì—… ë§í¬:</b> <a href='{company_link}' target='_blank'>{company_link}</a>")
            reply = "<br>".join(details)

        st.session_state.chat_messages.append({"role": "assistant", "content": reply, "timestamp": get_kst_time()})
        st.rerun()

    elif not is_relevant_question(user_input):
        reply = "â— ê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."
        st.session_state.chat_messages.append({"role": "assistant", "content": reply, "timestamp": get_kst_time()})
        st.rerun()

    else:
        
            # âœ… í›„ì† ì§ˆë¬¸ íŒë‹¨: ì´ì „ ì§ˆë¬¸ì´ ìˆì„ ë•Œë§Œ ìˆ˜í–‰
        if user_query_history:
            previous_input = user_query_history[-1]
            if not is_followup_question(previous_input, user_input):
                st.writer("ğŸ” [INFO] ë…ë¦½ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤. ê¸°ì¤€ ì„ë² ë”© ê°±ì‹ .")
                embedding_query_text = user_input
            else:
                st.writer("â¡ï¸ [INFO] í›„ì† ì§ˆë¬¸ì…ë‹ˆë‹¤. ê¸°ì¤€ ì„ë² ë”© ìœ ì§€.")
        else:
            st.writer("ğŸŒ± [INFO] ìµœì´ˆ ì§ˆë¬¸ì…ë‹ˆë‹¤. ê¸°ì¤€ ì„ë² ë”© ì„¤ì •.")
            embedding_query_text = user_input

        # ì‚¬ìš©ì ì…ë ¥ì„ ëŒ€í™” ì´ë ¥ê³¼ íˆìŠ¤í† ë¦¬ì— ê°ê° ì¶”ê°€
        #conversation_history.append({"role": "user", "content": user_input})
        user_query_history.append(user_input)  # âœ… ë¬´í•œ ì €ì¥ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        
        
        best_mode = is_best_recommendation_query(user_input)
        exclude = None if best_mode else st.session_state.excluded_keys
        results = recommend_services(user_input, exclude_keys=exclude, use_random=not best_mode)

        if not results:
            reply = "â— ê´€ë ¨ëœ ì¶”ì²œ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ ì£¼ì„¸ìš”."
            st.session_state.chat_messages.append({"role": "assistant", "content": reply, "timestamp": get_kst_time()})
            st.rerun()

        st.session_state.last_results = results
        st.session_state.all_results.append(results)

        context = make_context(results)
        prompt = make_prompt(user_input, context, is_best=best_mode)
        st.session_state.conversation_history.append({"role": "user", "content": prompt})
        gpt_reply = ask_gpt(st.session_state.conversation_history)
        st.session_state.conversation_history.append({"role": "assistant", "content": gpt_reply})
        st.session_state.chat_messages.append({"role": "assistant", "content": gpt_reply, "timestamp": get_kst_time()})

        mentioned_keys = parse_referenced_keys(gpt_reply, results)
        st.session_state.excluded_keys.update(mentioned_keys)
        st.rerun()