import streamlit as st
import faiss
import pickle
import numpy as np
import random
import itertools
from collections import deque
from openai import OpenAI

# ê¸°ë³¸ ì„¤ì •
SIMILARITY_THRESHOLD = 0.30
MAX_HISTORY_LEN = 10

# OpenAI Client (í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” .streamlit/secrets.tomlì— ì„¤ì •)
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
index = faiss.read_index("service_index.faiss")
with open("service_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

# ë²¡í„° ì •ê·œí™” ë° cosine ì¸ë±ìŠ¤ ì¤€ë¹„
xb = index.reconstruct_n(0, index.ntotal)
xb = xb / np.linalg.norm(xb, axis=1, keepdims=True)
d = xb.shape[1]
index_cosine = faiss.IndexFlatIP(d)
index_cosine.add(xb)

# Streamlit ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¦…ë‹ˆë‹¤:\n"
                                           "- ë‹µë³€ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ ë§íˆ¬ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
                                           "- ì§ˆë¬¸ì— ì¶”ì²œ/ì œì‹œ/ê²€ìƒ‰ ë“±ì˜ ë‹¨ì–´ê°€ ìˆë‹¤ë©´ ëª©ë¡ í˜•ì‹, ì•„ë‹ˆë©´ ì„œìˆ ì‹ìœ¼ë¡œ ë‹µí•´ ì£¼ì„¸ìš”.\n"
                                           "- ëª©ë¡ í˜•ì‹ì€ ë°˜ë“œì‹œ ë²ˆí˜¸ì™€ ëŒ€ì‹œ(-)ë§Œ ì‚¬ìš©í•˜ê³ , Markdown íŠ¹ìˆ˜ë¬¸ìëŠ” ì“°ì§€ ë§ˆì„¸ìš”.\n"
                                           "- ê¸°ì—…ëª…ì€ ë”°ì˜´í‘œë¡œ ë¬¶ê³ , ê¸°ì—…IDëŠ” ê´„í˜¸ë¡œ í‘œê¸°í•´ ì£¼ì„¸ìš”.\n"
                                           "- í•­ëª© ê°„ ê°œí–‰ ì—†ì´ ì¶œë ¥í•´ ì£¼ì„¸ìš”."}
    ]
if "all_results" not in st.session_state:
    st.session_state.all_results = deque(maxlen=MAX_HISTORY_LEN)
if "excluded_keys" not in st.session_state:
    st.session_state.excluded_keys = set()
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []

# ì„ë² ë”© ìƒì„± í•¨ìˆ˜
def get_embedding(text):
    response = client.embeddings.create(input=[text], model="text-embedding-3-small")
    return np.array(response.data[0].embedding).astype("float32").reshape(1, -1)

# GPT í˜¸ì¶œ í•¨ìˆ˜
def ask_gpt(messages):
    reply = client.chat.completions.create(model="gpt-4o", messages=messages)
    return reply.choices[0].message.content

# ì¶”ì²œ ì—¬ë¶€ íŒë³„
def is_best_recommendation_query(query):
    return any(k in query for k in ["ê°•ë ¥ ì¶”ì²œ", "ê°•ì¶”", "ì œì‹œ", "ì¶”ì²œ", "ì°¾ì•„", "ê²€ìƒ‰"])

# ìœ ì‚¬ë„ í•„í„°
def is_relevant_question(query, threshold=SIMILARITY_THRESHOLD):
    vec = get_embedding(query)
    D, _ = index_cosine.search(vec / np.linalg.norm(vec), 1)
    return D[0][0] >= threshold

# ì¶”ì²œ í•¨ìˆ˜
def recommend_services(query, top_k=5, exclude_keys=None):
    if exclude_keys is None:
        exclude_keys = set()
    vec = get_embedding(query)
    D, indices = index_cosine.search(vec / np.linalg.norm(vec), 300)

    ranked = [(score, metadata[idx]) for score, idx in zip(D[0], indices[0])]
    seen_keys, filtered = set(), []
    for score, s in ranked:
        key = (s['ê¸°ì—…ID'], s.get('ì„œë¹„ìŠ¤ìœ í˜•'), s.get('ì„œë¹„ìŠ¤ëª…'))
        if key in seen_keys or key in exclude_keys:
            continue
        seen_keys.add(key)
        filtered.append(s)
    return filtered[:top_k]

# ì¶œë ¥ìš© context ìƒì„±
def make_context(results):
    return "\n".join([
        f"{i+1}. \"{s['ì„œë¹„ìŠ¤ëª…']}\" - \"{s['ê¸°ì—…ëª…']}\" (ê¸°ì—…ID: {s['ê¸°ì—…ID']})\n"
        f"- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}\n"
        f"- ê¸ˆì•¡: {s.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')} / ê¸°í•œ: {s.get('ì„œë¹„ìŠ¤ê¸°í•œ', 'ì •ë³´ ì—†ìŒ')}"
        for i, s in enumerate(results)
    ])

# ìš”ì•½ìš© context

def make_summary_context(summary_memory):
    seen, deduped = set(), []
    for result_list in reversed(summary_memory):
        for item in result_list:
            key = (item['ì„œë¹„ìŠ¤ëª…'], item['ê¸°ì—…ëª…'])
            if key not in seen:
                seen.add(key)
                deduped.insert(0, item)
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}"
        for i, s in enumerate(deduped)
    ])

# í”„ë¡¬í”„íŠ¸ ìƒì„±
def make_prompt(query, context, is_best):
    extra = f"\nì§€ê¸ˆê¹Œì§€ ì¶”ì²œëœ ì„œë¹„ìŠ¤ ëª©ë¡:\n{make_summary_context(st.session_state.all_results)}" if is_best else ""
    return f"""
ë‹¹ì‹ ì€ ê´€ê´‘ìˆ˜í˜œê¸°ì—…ì—ê²Œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” AI ìƒë‹´ì‚¬ í˜¸ì¢…ì´ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ì§ˆë¬¸:
"{query}"

ì¶”ì²œ ê°€ëŠ¥í•œ ì„œë¹„ìŠ¤ ëª©ë¡:
{context}
{extra}
"""

# ---------- UI ì‹œì‘ ---------- #
st.markdown("""
<h1 style='text-align: center;'>ğŸ¯ í˜ì‹ ì´ìš©ê¶Œ ì„œë¹„ìŠ¤ íŒŒì¸ë”</h1>
<p style='text-align: center; font-size:14px;'>ğŸ¤– í˜¸ì¢…ì´ì—ê²Œ ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.</p>
""", unsafe_allow_html=True)

# ì±„íŒ… ì¶œë ¥
for msg in st.session_state.chat_messages:
    st.markdown(f"<div style='background:#f1f1f1; padding:10px; margin:10px 0; border-radius:6px;'>" +
                msg['content'].replace("\n", "<br>") + "</div>", unsafe_allow_html=True)

# ì‚¬ìš©ì ì…ë ¥
with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_area("", height=80, label_visibility="collapsed")
    submitted = st.form_submit_button("í˜¸ì¢…ì´ì—ê²Œ ë¬¼ì–´ë³´ê¸°")

if submitted and user_input.strip():
    st.session_state.chat_messages.append({"role": "user", "content": user_input})
    st.session_state.conversation_history.append({"role": "user", "content": user_input})

    if not is_relevant_question(user_input):
        msg = "â— ê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."
        st.session_state.chat_messages.append({"role": "assistant", "content": msg})
    else:
        best_mode = is_best_recommendation_query(user_input)
        exclude = None if best_mode else st.session_state.excluded_keys
        results = recommend_services(user_input, exclude_keys=exclude)

        if not best_mode:
            for s in results:
                st.session_state.excluded_keys.add((s['ê¸°ì—…ID'], s.get('ì„œë¹„ìŠ¤ìœ í˜•'), s.get('ì„œë¹„ìŠ¤ëª…')))

        st.session_state.all_results.append(results)
        context = make_context(results)
        prompt = make_prompt(user_input, context, is_best=best_mode)
        st.session_state.conversation_history.append({"role": "user", "content": prompt})

        gpt_reply = ask_gpt(st.session_state.conversation_history)
        st.session_state.conversation_history.append({"role": "assistant", "content": gpt_reply})
        st.session_state.chat_messages.append({"role": "assistant", "content": gpt_reply})

    st.rerun()
