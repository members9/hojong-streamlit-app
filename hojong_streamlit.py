
import streamlit as st
import streamlit.components.v1 as components
import faiss
import pickle
import numpy as np
import random
from collections import deque
import itertools
from datetime import datetime
from zoneinfo import ZoneInfo  # Python 3.9 ì´ìƒ
from sentence_transformers import SentenceTransformer
import time
import json

# âœ… ì§„ì… ì•”í˜¸ ì…ë ¥ ë¡œì§ (4ìë¦¬ ìˆ«ì ì˜ˆ: 7299)
if "authenticated" not in st.session_state:
    st.session_state.authenticated = False

if not st.session_state.authenticated:
    st.markdown("## ğŸ” ì ‘ê·¼ ê¶Œí•œ")
    password_input = st.text_input("4ìë¦¬ ìˆ«ì ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", type="password")
    if password_input and password_input.strip() == "7299":
        st.session_state.authenticated = True
        st.rerun()
    elif password_input:
        st.error("âŒ ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤.")
    st.stop()

# âœ… ì¸ì¦ ì´í›„ ì‹¤í–‰
st.set_page_config(layout="wide")

import openai
from openai import OpenAI
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# âœ… ìŠ¤íƒ€ì¼ ë° ë°˜ì‘í˜• CSS ì¶”ê°€
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap');
        
        html, body, .stApp {
            background-color: #FFFFFF !important;
            color: #0c0c0c !important;
            font-family: 'Noto Sans KR', sans-serif !important;
        }

        /* âœ… ì…ë ¥ì°½ */
        .stTextArea textarea {
            background-color: #f0f0f0 !important;
            color: #0c0c0c !important;
            border-radius: 6px !important;
            padding: 10px !important;
            border: 1px solid #DDDDDD !important;
        }

        /* âœ… ë²„íŠ¼ */
        .stButton > button {
            background-color: #0c0c0c !important;
            color: #FFFFFF !important;
            padding: 10px 16px !important;
            border-radius: 6px !important;
            border: 2px solid #FFFFFF !important;
        }

        /* âœ… ì‚¬ìš©ì/ì±—ë´‡ ë§í’ì„  */
        .user-msg-box {
            text-align: right !important;
        }
        .user-msg {
            display: inline-block !important;
            text-align: left !important; 
            background-color: #FFEB3B !important; 
            color: #0c0c0c !important;
            padding: 10px 14px !important; 
            border-radius: 12px 0px 12px 12px; 
            margin: 0 0 30px 0 !important; 
            max-width: 66% !important;
            word-wrap: break-word !important;
            overflow-wrap: break-word !important;
            word-break: break-all !important;
        }
        .user-msg-time {
            text-align: left !important;
            font-size: 11px;
            color: #666;
            margin-top: 2px;
            width: 100%;
        }    
        .chatbot-msg-box {
            text-align: left !important;
        }
        .chatbot-msg {
            display: inline-block !important; 
            text-align: left !important !important; 
            background-color: #bacee0 !important; 
            color: #0c0c0c !important !important;
            padding: 10px 14px !important; 
            border-radius: 0px 12px 12px 12px !important; 
            margin: 0 0 30px 0 !important; 
            max-width: 66% !important;
            word-wrap: break-word !important;
            overflow-wrap: break-word !important;
            word-break: break-all !important;
        }
        .chatbot-msg-time {
            text-align: right !important; 
            font-size: 11px;
            color: #666;
            margin-top: 2px;
            width: 100%;
        }   
        /* âœ… ê¸°íƒ€ */
        .responsive-title {
            font-size: clamp(40px, 5vw, 60px) !important;
            font-weight: 700 !important;
            color: #0c0c0c !important;  
            text-align: center !important;
            white-space: nowrap !important;
            overflow: hidden !important;
            width: 100% !important;
            padding-bottom: 2px !important;
            margin-top: -52px !important;
        }
        .responsive-subtitle {
            font-size: clamp(14px, 5vw, 12px) !important;
            color: #0c0c0c !important;  
            text-align: center !important;
            white-space: nowrap !important;
            overflow: hidden !important;
            width: 100% !important;
            padding-bottom: 2px !important;
        }
        .info-msg {
            background-color: #fff3cd !important; 
            font-size: 14px !important;
            border-left: 6px solid #ffeeba !important;
            border-radius: 6px 6px 6px 6px !important; 
            padding: 10px !important;
            margin-bottom: 10px !important;
            text-align: center !important;
        }    
        .user-guide {
            font-size: 14px !important;
            margin-top: 4px !important; 
            color: #0c0c0c !important; 
            text-align: left !important;
            line-height: 1.4 !important;
        }
        
        /* ë§í¬ ìŠ¤íƒ€ì¼ */
        .link-button {
            display: inline-block;
            background-color: #f8f9fa;
            color: #0366d6;
            padding: 5px 10px;
            border-radius: 4px;
            margin: 5px 0;
            text-decoration: none;
            border: 1px solid #ddd;
            white-space: normal;
            word-break: break-all;
            max-width: 100%;
        }
        
        @media screen and (max-width: 768px) {
            .input-row {
                flex-direction: column !important;
                align-items: stretch !important;
            }
            .user-msg, .chatbot-msg {
                font-size: 12px !important;
                line-height: 1.3 !important;
                margin: 0 0 15px 0 !important; 
            }
            .user-msg-time, chatbot-msg-time {
                font-size: 9x;
            }   
            .user-guide {
                font-size: 11px !important;
                line-height: 1.3 !important;
            }
            .info-msg {
                font-size: 11px !important;
                padding: 5px !important;
                margin-bottom: 5px !important;
            }
        }
        
        .main .block-container {
            padding-top: 1rem !important;
        }
    </style>
""", unsafe_allow_html=True)

# âœ… ì„¤ì • ë³€ìˆ˜ (13_service_recommender.pyì™€ ì¼ì¹˜í•˜ë„ë¡ ìœ ì§€)
USE_OPENAI_EMBEDDING = True  # ğŸ” ì—¬ê¸°ì„œ ìŠ¤ìœ„ì¹­ ê°€ëŠ¥ (True: OpenAI, False: ë¡œì»¬ ëª¨ë¸)
Q_SIMILARITY_THRESHOLD = 0.30
A_SIMILARITY_THRESHOLD = 0.45
MAX_HISTORY_LEN = 5  # ì§ˆë¬¸ê³¼ ë‹µë³€ íˆìŠ¤ë¡œë¦¬ ì €ì¥ ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜
FALLBACK_ATTEMPT_NUM = 2

# âœ… ì„¸ì…˜ ìƒíƒœì— ë””ë²„ê·¸ ëª¨ë“œ ë³€ìˆ˜ ì¶”ê°€
if "debug_mode" not in st.session_state:
    st.session_state.debug_mode = False

# # ì‚¬ì´ë“œë°”ì— ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€ ì¶”ê°€
# with st.sidebar:
#     st.title("ê°œë°œì ì„¤ì •")
#     debug_toggle = st.checkbox("ë””ë²„ê·¸ ëª¨ë“œ", value=st.session_state.debug_mode)
#     if debug_toggle != st.session_state.debug_mode:
#         st.session_state.debug_mode = debug_toggle
#         st.rerun()

# ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ í•¨ìˆ˜
def debug_info(message, level="info", pin=False):
        
    """ë””ë²„ê·¸ ëª¨ë“œì¼ ë•Œë§Œ í‘œì‹œ, í•€ ë©”ì‹œì§€ëŠ” ì…ë ¥ì°½ ìœ„ì— ê³ ì •"""
    if st.session_state.debug_mode:        
        if level == "info":
            st.info(message)
        elif level == "warning":
            st.warning(message)
        elif level == "error":
            st.error(message)
        elif level == "success":
            st.success(message)
        else:
            st.write(message)
    if pin:
        st.session_state.debug_pinned_message = message  # âœ… ê³ ì • ë©”ì‹œì§€ë¡œ ë“±ë¡

def pause_here(message="â¸ï¸ ë””ë²„ê¹… ì§€ì ì…ë‹ˆë‹¤. ê³„ì†í•˜ë ¤ë©´ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”."):
    if "pause_continue" not in st.session_state:
        st.session_state.pause_continue = False

    if not st.session_state.pause_continue:
        st.warning(message)
        if st.button("ğŸ‘‰ ê³„ì† ì‹¤í–‰í•˜ê¸°", key=f"btn_{len(st.session_state.chat_messages)}"):
            st.session_state.pause_continue = True
            st.rerun()
        else:
            st.stop()
            
# âœ… ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš” ì‹œ)
if not USE_OPENAI_EMBEDDING:
    local_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# âœ… KST ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
def get_kst_time():
    return datetime.now(ZoneInfo("Asia/Seoul")).strftime("%p %I:%M")

# âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = deque(maxlen=MAX_HISTORY_LEN + 1)  # system ë©”ì‹œì§€ í¬í•¨ì„ ìœ„í•´ +1
    st.session_state.conversation_history.append({
        "role": "system",
        "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤. "
                "[ì£¼ì˜: ëª¨ë“  ë‹µë³€ì€ ì•„ë˜ ì§€ì¹¨ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤]\n\n"
                "- ë‹µë³€ì€ ë°˜ë“œì‹œ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ìš”ì•½ìœ¼ë¡œ ì‹œì‘í•´ ì£¼ì„¸ìš”.\n"
                "- ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì‹œì‘í•´ ì£¼ì„¸ìš”: 'ì§ˆë¬¸ ìš”ì•½: \"...\"'\n"
                "- ì´í›„ ì´ì–´ì„œ ë³¸ë¬¸ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
                "- ëª©ë¡ìœ¼ë¡œ ì¶œë ¥í•  ê²½ìš° ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n"
                "\n"
                "  1. \"ì„œë¹„ìŠ¤ëª…\"\n"
                "     - \"ê¸°ì—…ëª…\" (ê¸°ì—…ID: XXXX)\n"
                "     - ìœ í˜•: ...\n"
                "     - ê¸ˆì•¡: ...\n"
                "     - ê¸°í•œ: ...\n"
                "     - ìš”ì•½: ...\n"
                "     -...\n"
                "\n"
                "- ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”.\n"
                "- ì ˆëŒ€ë¡œ ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼(ì˜ˆ: **êµµì€ ê¸€ì”¨**, *ê¸°ìš¸ì„*, ## ì œëª© ë“±)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."
                "- ì˜ˆ) ì˜ëª»ëœ ì˜ˆ: **\"ì‚¼ì„±ì „ì\"**, ##\"í™ˆí˜ì´ì§€ ì„œë¹„ìŠ¤\"##\n"
                "- ì˜ˆ) ì˜¬ë°”ë¥¸ ì˜ˆ: \"ì‚¼ì„±ì „ì\", \"í™ˆí˜ì´ì§€ ì„œë¹„ìŠ¤\"\n"
                "- ë¶ˆë¦¿ì€ í•­ìƒ ëŒ€ì‹œ(-)ë§Œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.\n"
                "- í•­ëª© ê°„ ê°œí–‰ì„ ë„£ì§€ ë§ˆì„¸ìš”.\n"
                "- ê¸°ì—…ëª…/ì„œë¹„ìŠ¤ëª…ì´ ì–¸ê¸‰ë˜ë©´ ë°˜ë“œì‹œ ì´ìœ ë„ í•¨ê»˜ ì œì‹œí•´ ì£¼ì„¸ìš”.\n"
                "- ê¸°ì—…IDëŠ” ë°˜ë“œì‹œ ê´„í˜¸ ì•ˆì— í‘œê¸°í•´ ì£¼ì„¸ìš”. ì˜ˆ: \"ì‚¼ì„±ì „ì\" (ê¸°ì—…ID: 12345)\n"
                "- ìœ„ ì§€ì¹¨ë“¤ì„ ì–´ê¸°ë©´ ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤."
    })
if "all_results" not in st.session_state:
    st.session_state.all_results = deque(maxlen=MAX_HISTORY_LEN)
if "excluded_keys" not in st.session_state:
    st.session_state.excluded_keys = set()
if "last_results" not in st.session_state:
    st.session_state.last_results = []
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []
if "embedding_cache" not in st.session_state:
    st.session_state.embedding_cache = {}
if "followup_cache" not in st.session_state:
    st.session_state.followup_cache = {}
if "user_query_history" not in st.session_state:
    st.session_state.user_query_history = []
if "embedding_query_text" not in st.session_state:
    st.session_state.embedding_query_text = None
if "embedding_query_text_summary" not in st.session_state:
    st.session_state.embedding_query_text_summary = None
if "embedding_query_vector" not in st.session_state:
    st.session_state.embedding_query_vector = None  # ë²¡í„° ìºì‹± ì´ˆê¸°í™”
if "pending_fallback" not in st.session_state:
    st.session_state.pending_fallback = False
if "fallback_attempt" not in st.session_state:
    st.session_state.fallback_attempt = 0
if "A_SIMILARITY_THRESHOLD" not in st.session_state:
    st.session_state.A_SIMILARITY_THRESHOLD = A_SIMILARITY_THRESHOLD  # ê¸°ë³¸ê°’ ì‚¬ìš©
if "TOP_N" not in st.session_state:
    st.session_state.TOP_N = MAX_HISTORY_LEN
if "is_processing" not in st.session_state:
    st.session_state.is_processing = False    


# âœ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def normalize(vecs):
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    return vecs / norms

# âœ… FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
@st.cache_resource
def load_index_and_metadata():
    index = faiss.read_index("service_index.faiss")
    with open("service_metadata.pkl", "rb") as f:
        metadata = pickle.load(f)
    
    # ì €ì¥ëœ ì„ë² ë”© ë²¡í„°ë¥¼ ì¬êµ¬ì„±í•˜ê³  ì •ê·œí™”
    xb = index.reconstruct_n(0, index.ntotal)
    xb = normalize(xb)
    d = xb.shape[1]
    index_cosine = faiss.IndexFlatIP(d) 
    index_cosine.add(xb)
    
    return index, metadata, index_cosine

# ê¸°ì—… IDë¥¼ í‚¤ë¡œ, ê¸°ì—…ëª…ì„ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
@st.cache_resource
def create_company_lookup():
    company_dict = {}
    for item in metadata:
        if "ê¸°ì—…ID" in item and "ê¸°ì—…ëª…" in item:
            company_dict[str(item["ê¸°ì—…ID"])] = item["ê¸°ì—…ëª…"]
    return company_dict

# âš ï¸ ì´ í˜¸ì¶œì€ í•¨ìˆ˜ ì •ì˜ í›„ì— ë°°ì¹˜
index, metadata, index_cosine = load_index_and_metadata()
company_lookup = create_company_lookup()

def get_embedding(text, model="text-embedding-3-small"):
    if text in st.session_state.embedding_cache:
        return st.session_state.embedding_cache[text]

    if USE_OPENAI_EMBEDDING:
        response = client.embeddings.create(input=[text], model=model)
        embedding = response.data[0].embedding  # ìˆ˜ì •ëœ ë¶€ë¶„: ë”•ì…”ë„ˆë¦¬ ì ‘ê·¼ì´ ì•„ë‹Œ ê°ì²´ ì†ì„± ì ‘ê·¼
    else:
        embedding = local_model.encode([text])[0].tolist()

    st.session_state.embedding_cache[text] = embedding
    return embedding

def is_followup_question(prev, current):
    key = (prev.strip(), current.strip())  # ì „ì²˜ë¦¬ëœ ì§ˆë¬¸ ìŒì„ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©

    if key in st.session_state.followup_cache:
        return st.session_state.followup_cache[key]

    messages = [
        {"role": "system", "content": """ë‹¹ì‹ ì€ AI ì§ˆë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
                ë‹¤ìŒì— ì œì‹œëœ ë‘ ì§ˆë¬¸ì„ ë¹„êµí•´ì„œ, ë‘ ë²ˆì§¸ ì§ˆë¬¸ì´ ì²« ë²ˆì§¸ ì§ˆë¬¸ì˜ ì‘ë‹µì„ ì „ì œë¡œ í•œ í›„ì† ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•´ ì£¼ì„¸ìš”.\n
                - í›„ì† ì§ˆë¬¸ì´ë€ ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ **ì¶”ê°€ ìš”ì²­**, **ê´€ë ¨ ì¡°ê±´ì˜ í™•ëŒ€**, **êµ¬ì²´í™”**, **ê³„ì†ëœ íƒìƒ‰** ë“±ì´ í¬í•¨ëœ ê²½ìš°ì…ë‹ˆë‹¤.\n
                - ë‹¨ìˆœíˆ ìœ ì‚¬í•˜ê±°ë‚˜ í‚¤ì›Œë“œê°€ ê°™ì€ ê²ƒì´ ì•„ë‹ˆë¼ **ì „ì œ ë§¥ë½ ì—†ì´ ì˜ë¯¸ê°€ ë¶€ì¡±í•œ ë¬¸ì¥**ë„ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.\n
                - yes ë˜ëŠ” noë¡œë§Œ ë‹µí•´ ì£¼ì„¸ìš”."""},
        {"role": "user", "content": f"ì´ì „ ì§ˆë¬¸: {prev}\ní˜„ì¬ ì§ˆë¬¸: {current}"}
    ]
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=messages
        )
        answer = response.choices[0].message.content.strip().lower() 
        result = "yes" in answer  # 'yes' í¬í•¨ ì—¬ë¶€ë¡œ íŒë‹¨
        st.session_state.followup_cache[key] = result  # âœ… ìºì‹œ ì €ì¥
        return result
    except Exception as e:
        return True  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì€ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼
    
    
# âœ… ìš”ì•½ í•¨ìˆ˜ ì¶”ê°€ (GPT ì‚¬ìš©)
def summarize_query(query):
    """
    ê¸´ ì‚¬ìš©ì ì§ˆë¬¸ì„ ìœ ì‚¬ë„ ì„ë² ë”©ì— ì í•©í•˜ë„ë¡ ìš”ì•½
    """
    prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{query}\n\n
                ì´ ì§ˆë¬¸ì„ ë²¡í„° ì„ë² ë”©ì— ì í•©í•˜ë„ë¡ í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”. 
                ë¶ˆí•„ìš”í•œ ì„œì‚¬ë‚˜ ì˜ˆì‹œëŠ” ì œê±°í•˜ê³ , í•µì‹¬ ëª©ì /ì¡°ê±´/í¬ë§ì‚¬í•­ë§Œ ì •ë¦¬í•´ ì£¼ì„¸ìš”.
                ì¶œë ¥ì€ (ì§ˆë¬¸ì˜ ì´ ê¸¸ì´ë¥¼ 100ìœ¼ë¡œ ë‚˜ëˆˆ ìˆ˜)ë§Œí¼ì˜ ë¬¸ì¥ ìˆ˜ë¡œ ì‘ì„± í•´ì£¼ì„¸ìš”."""
    
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ AI ì§ˆì˜ ìš”ì•½ ë„ìš°ë¯¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        debug_info(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {str(e)}", level="error")
        return query  # ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ì‚¬ìš©

# âœ… get_embedding í˜¸ì¶œ ì „ ìš”ì•½ ì²˜ë¦¬ (ë²¡í„° ê²€ìƒ‰ ì´ì „)
def get_embedding_with_optional_summary(text, model="text-embedding-3-small"):
    # ë„ˆë¬´ ê¸´ ê²½ìš°ë§Œ ìš”ì•½
    if len(text) > 150:
        debug_info("ğŸ“Œ ì§ˆë¬¸ì´ ê¸¸ì–´ GPTë¡œ ìš”ì•½ í›„ ë²¡í„°í™”í•©ë‹ˆë‹¤.", pin=True)
        text = summarize_query(text)
        st.session_state.embedding_query_text_summary = text
        debug_info(f"ğŸ“Œ gpt ìš”ì•½: " + text)
    return get_embedding(text, model)

def is_best_recommendation_query(query):
    keywords = ["ê°•ë ¥ ì¶”ì²œ", "ê°•ì¶”"]
    return any(k in query for k in keywords)

def is_relevant_question(query, threshold=Q_SIMILARITY_THRESHOLD): 
    query_vec = get_embedding_with_optional_summary(query)
    st.session_state.embedding_query_vector = query_vec
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec = normalize(query_vec)
    D, _ = index_cosine.search(query_vec, 1)
    max_similarity = D[0][0]
    return max_similarity >= threshold

def is_related_results_enough(ranked_results, threshold=A_SIMILARITY_THRESHOLD, top_n=MAX_HISTORY_LEN):
    """
    ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ì¶”ì²œ ê²°ê³¼ ì¤‘ ìƒìœ„ Nê°œì˜ í‰ê·  ìœ ì‚¬ë„ê°€ threshold ì´ìƒì¸ì§€ í™•ì¸.
    ê´€ë ¨ë„ê°€ ë‚®ìœ¼ë©´ False ë°˜í™˜ â†’ GPT í˜¸ì¶œ ë°©ì§€ ê°€ëŠ¥.
    """
    threshold = threshold or st.session_state.A_SIMILARITY_THRESHOLD
    top_n = top_n or st.session_state.TOP_N
    debug_info(f"ğŸ“Œ threshold : " + str(threshold))
    debug_info(f"ğŸ“Œ top_n : " + str(top_n))
    if not ranked_results or len(ranked_results) < top_n:
        debug_info(f"ğŸ“Œ ê´€ë ¨ë„ê°€ ë‚®ìœ¼ë©´ False ë°˜í™˜ â†’ GPT í˜¸ì¶œ ë°©ì§€ ê°€ëŠ¥.")
        return False
    top_scores = [score for score, _ in ranked_results[:top_n]]
    avg_score = sum(top_scores) / len(top_scores)
    debug_info(f"ğŸ¤– ë¶„ì„ ê²°ê³¼ ìƒìœ„ {top_n}ê°œ í‰ê·  ìœ ì‚¬ë„ëŠ” {avg_score:.4f} ì…ë‹ˆë‹¤.", pin=True)
    
    return avg_score >= threshold

def recommend_services(query, top_k=5, exclude_keys=None, use_random=True):
    # âœ… exclude_keysê°€ Noneì´ë©´ ë¹ˆ ì§‘í•©ìœ¼ë¡œ ì´ˆê¸°í™”
    if exclude_keys is None:
        exclude_keys = set()
    
    # ë²¡í„°ë¥¼ ì¬ì‚¬ìš©í•˜ë ¤ í–ˆìœ¼ë‚˜, í•˜ê¸° ì´ìœ  ë°œìƒìœ¼ë¡œ ì¬ì‚¬ìš© ì•ˆí•¨.
    # ì´ì „ ì§ˆë¬¸: í™ˆí˜ì´ì§€ êµ¬ì¶• ì—…ì²´ ì•Œë ¤ì¤˜ â†’ ë²¡í„° A
    # ì§€ê¸ˆ ì§ˆë¬¸: ë””ìì¸ ì—…ì²´ë„ ì•Œë ¤ì¤˜ â†’ ë²¡í„° A ê·¸ëŒ€ë¡œ ì‚¬ìš©
    # â†’ "ë””ìì¸"ì´ ê°•ì¡°ë˜ì–´ì•¼ í•  í…ìŠ¤íŠ¸ì— "í™ˆí˜ì´ì§€" ë²¡í„°ë¥¼ ì“°ê²Œ ë¨
    # if "embedding_query_vector" in st.session_state and st.session_state.embedding_query_vector is not None:
    #     debug_info(f"\nğŸ“Œ ì´ì „ ìƒì„±ëœ ë²¡í„° ì¬ì‚¬ìš©")
    #     query_vec = st.session_state.embedding_query_vector
    # else:
    #     debug_info(f"\nğŸ“Œ ìƒˆë¡œìš´ ì¿¼ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìš”ì•½ í›„ ìƒˆë¡œìš´ ë²¡í„° ìƒì„±")

    query_vec = get_embedding_with_optional_summary(query)
    st.session_state.embedding_query_vector = query_vec
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec = normalize(query_vec)

    # âœ… 2. ìœ ì‚¬í•œ ì„œë¹„ìŠ¤ 300ê°œ ê²€ìƒ‰ (ê¸°ì¡´ 100ê°œ â†’ í™•ì¥)
    D, indices = index_cosine.search(query_vec, 300)

    # 3. ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œë¡œ (score, service) ëª©ë¡ ìƒì„±
    ranked = [(score, metadata[idx]) for score, idx in zip(D[0], indices[0])]
    
    # ğŸ“Œ STEP 1: ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ëœ ì›ë³¸ ìƒìœ„ 30ê°œ ì¶œë ¥
    debug_info(f"\nğŸ“Œ [STEP 1] ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬ëœ ì›ë³¸ ìƒìœ„ 30ê°œ:")
    for i, (score, s) in enumerate(ranked[:30]):
        debug_info(f"{i+1}. [{score:.4f}] {s['ê¸°ì—…ëª…']} / {s.get('ì„œë¹„ìŠ¤ìœ í˜•')} / {s.get('ì„œë¹„ìŠ¤ëª…')}")
    
    # â›” ìœ ì‚¬ë„ ë‚®ì„ ê²½ìš° GPT í˜¸ì¶œë„ ìƒëµí•  ìˆ˜ ìˆë„ë¡ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    debug_info(f"âœ… íŒŒë¼ë¯¸í„° ì¡°ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸: ì„ê³„ê°’={st.session_state.A_SIMILARITY_THRESHOLD}, TOP_N={st.session_state.TOP_N}", "success")
    if not is_related_results_enough(ranked, st.session_state.A_SIMILARITY_THRESHOLD, st.session_state.TOP_N):
        debug_info("ğŸ“Œ ì¶”ì²œ ê²°ê³¼ì˜ ì—°ê´€ì„±ì´ ë‚®ì•„ fallback ë£¨í”„ë¡œ ì§„ì…í•©ë‹ˆë‹¤.", "warning")
        st.session_state.pending_fallback = True
        return []
    

        
    # âœ… 4. ì œì™¸í•  í‚¤ (ê¸°ì—…ID + ì„œë¹„ìŠ¤ìœ í˜• + ì„œë¹„ìŠ¤ëª…) ì •ì˜
    if exclude_keys:
        debug_info(f"\n\nğŸ“Œ [STEP 2] ì œì™¸ ëŒ€ìƒ í‚¤ ìˆ˜: {len(exclude_keys)}")
        for i, key in enumerate(list(exclude_keys)[:10]):
            company_name = company_lookup.get(str(key[0]), "ì•Œ ìˆ˜ ì—†ìŒ")
            debug_info(f" - ì œì™¸ {i+1}: ê¸°ì—…ID={key[0]} / ê¸°ì—…ëª…={company_name} / {key[1]} / {key[2]}")
    else:
        debug_info("\n\nğŸ“Œ [STEP 2] ì œì™¸ ëŒ€ìƒ ì—†ìŒ")

    # 4. ì¤‘ë³µ ì œê±° ë° ì œì™¸ ëŒ€ìƒ í•„í„°ë§
    seen_keys = set()
    filtered = []
    for score, service in ranked:
        key = (service["ê¸°ì—…ID"], service.get("ì„œë¹„ìŠ¤ìœ í˜•"), service.get("ì„œë¹„ìŠ¤ëª…"))
        if key in exclude_keys or key in seen_keys:
            continue
        seen_keys.add(key)
        filtered.append((score, service))

    # 5. ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë¼ ìˆìœ¼ë‚˜ ì•ˆì „ ì°¨ì›ì—ì„œ ì¬ì •ë ¬)
    filtered.sort(key=lambda x: x[0], reverse=True)
    
    # âœ… ìƒìœ„ 30ê°œê¹Œì§€ ì¶œë ¥ (ë””ë²„ê¹… ë˜ëŠ” ë¡œê·¸ í™•ì¸ìš©)
    debug_info(f"\nğŸ“Œ [STEP 3] í•„í„°ë§ í›„ ìƒìœ„ 30ê°œ:")
    for i, (score, s) in enumerate(filtered[:30]):
        debug_info(f"{i+1}. [{score:.4f}] {s['ê¸°ì—…ëª…']} / {s.get('ì„œë¹„ìŠ¤ìœ í˜•')} / {s.get('ì„œë¹„ìŠ¤ëª…')}")

    # 6. ìƒìœ„ 10ê°œ ì¤‘ ëœë¤ ì„ íƒ or top_kê°œ ì„ íƒ
    if use_random:
        top_10 = filtered[:10]
        selected = random.sample(top_10, min(len(top_10), top_k))
        results = [service for _, service in selected]
    else:
        results = [service for _, service in filtered[:top_k]]

    return results

def ask_gpt(messages):
    response = client.chat.completions.create(model="gpt-4o", messages=messages)
    return response.choices[0].message.content

def make_context(results):
    """ì¶”ì²œ ê²°ê³¼ ëª©ë¡ì„ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë„ë¡ êµ¬ì„± (ê¸°ì—…ì˜ ìƒì„¸ì •ë³´ í¬í•¨)"""
    return "\n".join([
        f"{i+1}. \"{s['ì„œë¹„ìŠ¤ëª…']}\" (ì œê³µê¸°ì—…: \"{s['ê¸°ì—…ëª…']}\", ê¸°ì—…ID: {s['ê¸°ì—…ID']})\n"
        f"- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}\n"
        f"- ê¸ˆì•¡: {s.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')} / ê¸°í•œ: {s.get('ì„œë¹„ìŠ¤ê¸°í•œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ë²•ì¸ì—¬ë¶€: {s.get('ê¸°ì—…ì˜ ë²•ì¸ì—¬ë¶€', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìœ„ì¹˜: {s.get('ê¸°ì—… ìœ„ì¹˜', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•µì‹¬ì—­ëŸ‰: {s.get('ê¸°ì—… í•µì‹¬ì—­ëŸ‰', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- 3ê°œë…„ í‰ê·  ë§¤ì¶œ: {s.get('ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•´ë‹¹ë¶„ì•¼ì—…ë ¥: {s.get('ê¸°ì—… í•´ë‹¹ë¶„ì•¼ì—…ë ¥', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì£¼ìš”ì‚¬ì—…ë‚´ìš©: {s.get('ê¸°ì—… ì£¼ìš”ì‚¬ì—…ë‚´ìš©', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì¸ë ¥í˜„í™©: {s.get('ê¸°ì—… ì¸ë ¥í˜„í™©', 'ì •ë³´ ì—†ìŒ')}"
        for i, s in enumerate(results)
    ])

def make_summary_context(summary_memory):
    seen = set()
    deduplicated = []
    for result_list in reversed(summary_memory):
        for item in result_list:
            key = (item['ì„œë¹„ìŠ¤ëª…'], item['ê¸°ì—…ëª…'], item.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì—†ìŒ'))
            if key not in seen:
                seen.add(key)
                deduplicated.insert(0, item)  # ì›ë˜ ìˆœì„œ ìœ ì§€
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}"
        for i, s in enumerate(deduplicated)
    ])

def make_prompt(query, context, is_best=False):
    summarized = st.session_state.embedding_query_text_summary or query
    if is_best:
        history = make_summary_context(st.session_state.all_results)
        extra = f"ì§€ê¸ˆê¹Œì§€ ì¶”ì²œí•œ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{history}\n\nì´ì „ì— ì¶”ì²œëœ ê¸°ì—…ë„ í¬í•¨í•´ì„œ ì¡°ê±´ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ìµœê³ ì˜ ì¡°í•©ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
    else:
        extra = ""
        
    return f"""[ì£¼ì˜: ì•„ë˜ ì§€ì¹¨ì— ë”°ë¼ ì ˆëŒ€ ë§ˆí¬ë‹¤ìš´ì´ë‚˜ ê°•ì¡° í‘œì‹œ ì—†ì´ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.]

ë‹¹ì‹ ì€ ê´€ê´‘ìˆ˜í˜œê¸°ì—…ì—ê²Œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.

ì§ˆë¬¸ ìš”ì•½: "{summarized}" â† ë°˜ë“œì‹œ ì´ ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€ì„ ì‹œì‘í•´ ì£¼ì„¸ìš”.

ê·¸ë¦¬ê³  ê´€ë ¨ëœ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:
{context}

ğŸ“Œ {extra}
"""

# âœ… UI ì¶œë ¥ ì˜ì—­
st.markdown("""
    <div class="responsive-title">í˜ì‹ ë°”ìš°ì²˜ ì„œë¹„ìŠ¤ íŒŒì¸ë”</div>
    <p class="responsive-subtitle">ğŸ¤– í˜¸ì¢…ì´ì—ê²Œ ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.</p>
""", unsafe_allow_html=True)

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.chat_messages:
    if msg["role"] == "user":
        st.markdown(f"""
        <div class="user-msg-box">
            <div class="user-msg">
                {msg["content"].replace(chr(10), "<br>")}
                <div class="user-msg-time">{msg['timestamp']}</div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="chatbot-msg-box">
            <div class="chatbot-msg"> 
                {msg["content"].replace(chr(10), "<br>")}
                <div class="chatbot-msg-time">{msg['timestamp']}</div>
            </div>
        </div>
        """, unsafe_allow_html=True)

# if st.session_state.get("is_processing", False):
if "debug_pinned_message" in st.session_state:
    st.markdown(f"""
        <div class="info-msg">
            "{st.session_state.debug_pinned_message}"
        </div>
    """, unsafe_allow_html=True)

# ì…ë ¥ í¼
with st.form("chat_form", clear_on_submit=True):
    st.markdown("<div class='input-row'>", unsafe_allow_html=True)
    user_input = st.text_area("", height=100, label_visibility="collapsed")
    submitted = st.form_submit_button("ë¬¼ì–´ë³´ê¸°")
    st.markdown("</div>", unsafe_allow_html=True)

# ì‚¬ìš© ì•ˆë‚´ í‘œì‹œ
st.markdown("""
    <div class="user-guide">
        â„¹ï¸ ì‚¬ìš©ë²• ì•ˆë‚´:<br>
        â€¢&nbsp;<b>"ìì„¸íˆ ê¸°ì—…ëª… ì„œë¹„ìŠ¤ëª…"</b>ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”. (ê¸°ì—…ëª…ê³¼ ì„œë¹„ìŠ¤ëª…ì€ ë‹¨ì–´ ì¼ë¶€ë§Œ ì…ë ¥í•˜ì…”ë„ ë˜ìš”.)<br>
        â€¢&nbsp;<b>"ê°•ë ¥ ì¶”ì²œ"</b> ì„ í¬í•¨í•˜ì—¬ ì…ë ¥í•˜ë©´ ì•ì„œ ì œì‹œëœ ì„œë¹„ìŠ¤ë“¤ì„ í¬í•¨í•œ ì „ì²´ ì¶”ì²œì„ ë°›ì•„ë³¼ ìˆ˜ ìˆì–´ìš”.<br>
        â€¢&nbsp;<b>"ì´ˆê¸°í™”"</b>ë¥¼ ì…ë ¥í•˜ë©´ ëŒ€í™” ë‚´ìš©ê³¼ ì¶”ì²œ ê¸°ë¡ì„ ëª¨ë‘ ì§€ìš¸ ìˆ˜ ìˆì–´ìš”.<br>
        â€¢&nbsp;<b>"ë³µí•©ì ì¸ ì¡°ê±´ë“¤"</b>ì„ ì´ìš©í•œ ì§ˆë¬¸ì´ë‚˜ <b>"ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì •ë³´"</b> ê²€ìƒ‰ìœ¼ë¡œ í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•´ ë³´ì„¸ìš”.<br>
        - ì˜ˆë¥¼ë“¤ì–´ "ìš°ë¦¬ íšŒì‚¬ëŠ” ì™¸êµ­ì¸ì—ê²Œ êµ­ë‚´ ìœ ëª… ê´€ê´‘ì§€ì—­ì„ ì†Œê°œí•˜ê³  ìˆ™ë°•ì„ ì—°ê²°í•´ì£¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ í•˜ê³  ìˆì–´. íšŒì‚¬ í™ˆí˜ì´ì§€ë¥¼ ë””ìì¸ ì¤‘ì‹¬ìœ¼ë¡œ ê°œí¸í•˜ê³  ì‹¶ê³ , ì°¸ ë‹¤êµ­ì–´ëŠ” í•„ìˆ˜ê³ , ìˆ™ë°•ì§€ë¥¼ ì˜ˆì•½í•˜ê³  ê²°ì œí•˜ëŠ” ì‡¼í•‘ëª° ê¸°ëŠ¥ì´ ë°˜ë“œì‹œ í•„ìš”í•´. ë˜í•œ ì¸ìŠ¤íƒ€ê·¸ë¨ìœ¼ë¡œ í™ë³´ë„ ì˜ í•˜ëŠ” ê²ƒë„ í•„ìˆ˜ê³ . ì´ëŸ°ê±¸ ë§Œì¡±ì‹œí‚¬ ìˆ˜ ìˆëŠ” ì¡°í•©ì„ ë§Œë“¤ì–´ì¤˜. ë‹¨, ì˜ˆì‚°ì€ í•©ì³ì„œ 5,000ë§Œì›ê¹Œì§€ì´ê³ , ê¸°ê°„ì€ 3.5ê°œì›”ì•ˆì—ëŠ” ë§ˆì³ì•¼ í•´. ë§ì€ ì†Œí†µì„ ìœ„í•´ ê°€ê¸‰ì  ìˆ˜ë„ê¶Œ ì§€ì—­ì— ìˆëŠ” íšŒì‚¬ì˜€ìœ¼ë©´ ì¢‹ê² ê³ , ë§¤ì¶œë„ 30ì–µ ì´ìƒë˜ë©° ì¸ì›ë„ ë§ì•„ì„œ ì•ˆì •ì ì¸ ì§€ì›ë„ ë°›ì•˜ìœ¼ë©´ í•˜ê³ . ì´ëŸ° íšŒì‚¬ë“¤ë¡œ ì°¾ì•„ë´ì¤˜. ë˜ ì–´ë–»ê²Œ ì´ë“¤ì„ ì¡°í•©í•˜ë©´ ë˜ëŠ”ì§€, ì™œ ì¶”ì²œí–ˆëŠ”ì§€ë„ ìƒì„¸íˆ ì•Œë ¤ì¤˜.<br>
        - ë˜ëŠ” "ì‚¼ì„±ì „ìì™€ ìœ ì‚¬í•œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” ë‹¤ë¥¸ ê¸°ì—…ì„ ì¶”ì²œí•´ì¤˜." 
    </div>
""", unsafe_allow_html=True)


if st.session_state.get("is_processing", False):
    user_input = st.session_state.pending_input
    del st.session_state.pending_input
    submitted = True
    # ì˜ë„ì ì¸ ë”œë ˆì´ (ë²¡í„°ì—ì„œë§Œ ê²€ìƒ‰í•´ì„œ ë„ˆë¬´ ë¹ ë¥´ë‹ˆ ì°¾ì•„ë³´ëŠ”ê±° ê°™ì§€ ì•Šì•„ì„œ)
    if st.session_state.pending_fallback:
        time.sleep(1)

# ë©”ì‹œì§€ ì²˜ë¦¬ ë¡œì§
if submitted and user_input.strip():
    
    # ì‹œê°„ëŒ€ ì„¤ì •
    current_time = get_kst_time()
    
    if not st.session_state.get("is_processing", False):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì•„ì˜ˆ ì—¬ê¸°ì„œ ì €ì¥í•´ë²„ë¦¼ (ê·¸ë˜ì•¼ ë²„íŠ¼ ëˆ„ë¥´ê³  ë°”ë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŒ.)
        st.session_state.chat_messages.append({"role": "user", "content": user_input, "timestamp": current_time})
        
        st.session_state.pending_input = user_input
        st.session_state.is_processing = True  # ë¶„ì„ ì¤‘ ìƒíƒœ True ì„¤ì •
        debug_info("ğŸ¤– ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”. ìµœì ì˜ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...", pin=True)
        st.rerun()
    else:
        debug_info("ğŸ¤– ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”. ìµœì ì˜ ë‹µë³€ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤...", pin=True)
        st.session_state.is_processing = False  
    
    # âœ… fallback ìƒí™©ì¸ì§€ ë¨¼ì € ì²´í¬í•˜ê³ , ì‚¬ìš©ì ì…ë ¥ì„ ì•„ì§ ì €ì¥í•˜ì§€ ì•ŠìŒ
    if st.session_state.pending_fallback:
        debug_info("ğŸ“š fallback ìƒíƒœ ê°ì§€ë¨ : " + str(st.session_state.fallback_attempt), "success")
        
        if user_input.strip().lower() == "ë„¤" and st.session_state.fallback_attempt < FALLBACK_ATTEMPT_NUM:
            # íŒŒë¼ë¯¸í„° ì¡°ì •
            st.session_state.fallback_attempt += 1
            st.session_state.A_SIMILARITY_THRESHOLD = max(0.1, st.session_state.A_SIMILARITY_THRESHOLD - 0.03)
            st.session_state.TOP_N = max(2, st.session_state.TOP_N - 1)
            
            # ì´ì œ ì‚¬ìš©ì ì…ë ¥ ì €ì¥
            # st.session_state.chat_messages.append({
            #     "role": "user",
            #     "content": user_input,
            #     "timestamp": current_time
            # })
            
            debug_info(f"ğŸ“š íŒŒë¼ë¯¸í„° ì¡°ì •ë¨: ì„ê³„ê°’={st.session_state.A_SIMILARITY_THRESHOLD}, TOP_N={st.session_state.TOP_N}", "success")
            
            # ì´ì „ ì§ˆë¬¸ìœ¼ë¡œ ê¸°ì¤€ ì„ë² ë”© ë³µì›
            #if st.session_state.user_query_history:
            #    st.session_state.embedding_query_text += ("," + st.session_state.user_query_history[-1])
            
            debug_info(f"ğŸ“š embedding_query_text : " + str(st.session_state.embedding_query_text))
            # pause_here("ğŸ§ª 001 last_results : " + str(st.session_state.embedding_query_text))
            
            # ê²€ìƒ‰ ë¡œì§ ì§ì ‘ ì‹¤í–‰
            best_mode = is_best_recommendation_query(st.session_state.embedding_query_text)
            
            # pause_here("ğŸ§ª 002 best_mode : " + str(best_mode))
            
            exclude = None if best_mode else st.session_state.excluded_keys
            last_results = recommend_services(
                st.session_state.embedding_query_text,
                exclude_keys=exclude,
                use_random=not best_mode
            )
            
            # pause_here("ğŸ§ª 003 last_results : " + str(last_results))
            
            # ê²°ê³¼ ì²˜ë¦¬
            if not last_results:
                # ì—¬ì „íˆ ê²°ê³¼ê°€ ì—†ìŒ - ë‹¤ì‹œ fallback ìƒíƒœë¡œ
                st.session_state.pending_fallback = True
                if st.session_state.fallback_attempt == 1:
                    reply = "âš ï¸ ì¡°ê¸ˆ ë” í¬ê´„ì ì¸ ë²”ìœ„ë¡œ ë‹¤ì‹œ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤. ì§„í–‰ì„ ì›í•˜ì‹œë©´ 'ë„¤' ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”."
                elif st.session_state.fallback_attempt == 2:
                    reply = "âš ï¸ ì—¬ì „íˆ ì„œë¹„ìŠ¤ë¥¼ ì°¾ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ ë” ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤. ì§„í–‰ì„ ì›í•˜ì‹œë©´ 'ë„¤' ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”."
                else:
                    reply = "âš ï¸ ì •ë³´ ì œê³µì´ ë¶ˆê°€ëŠ¥í•˜ì—¬ ì£„ì†¡í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì‹œë©´ ë‹¤ì‹œ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤."        
                st.session_state.chat_messages.append({
                    "role": "assistant", 
                    "content": reply, 
                    "timestamp": current_time
                })
                debug_info(f"ğŸ¤– ...", pin=True)
                # pause_here("ğŸ§ª 004-1 last_results is null")/
                # st.rerun()
            else:
                
                # pause_here("ğŸ§ª 004-2 last_results is not null") 
                
                # ê²°ê³¼ ì°¾ìŒ - ì²˜ë¦¬ ì§„í–‰
                context = make_context(last_results)
                gpt_prompt = make_prompt(st.session_state.embedding_query_text, context, is_best=best_mode)
                
                # GPT í˜¸ì¶œ
                st.session_state.conversation_history.append({"role": "user", "content": gpt_prompt})
                try:
                    gpt_reply = ask_gpt(list(st.session_state.conversation_history))
                    # pause_here("ğŸ§ª 005-1 gpt_reply : " + gpt_reply)
                    
                except Exception as e:
                    gpt_reply = f"âš ï¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”: {str(e)}"
                    # pause_here("ğŸ§ª 005-2 gpt_reply is error! ")
                    
                # ì‘ë‹µ ì €ì¥
                st.session_state.conversation_history.append({"role": "assistant", "content": gpt_reply})
                
                # ê²°ê³¼ ì²˜ë¦¬
                mentioned_keys = {
                    (s["ê¸°ì—…ID"], s.get("ì„œë¹„ìŠ¤ìœ í˜•"), s.get("ì„œë¹„ìŠ¤ëª…"))
                    for s in last_results
                    if (
                        str(s["ê¸°ì—…ID"]) in gpt_reply and
                        s["ì„œë¹„ìŠ¤ëª…"] in gpt_reply
                    )
                }
                
                # ì œì™¸ ëŒ€ìƒ ì—…ë°ì´íŠ¸
                st.session_state.excluded_keys.update(mentioned_keys)
                
                # ê²°ê³¼ ì €ì¥
                st.session_state.last_results = last_results
                st.session_state.all_results.append(last_results)
                
                # ì±—ë´‡ ì‘ë‹µ ì¶”ê°€
                st.session_state.chat_messages.append({
                    "role": "assistant", 
                    "content": gpt_reply, 
                    "timestamp": current_time
                })
                # fallback ìƒíƒœ ì´ˆê¸°í™”
                st.session_state.pending_fallback = False
                st.session_state.fallback_attempt = 0
                st.session_state.A_SIMILARITY_THRESHOLD = A_SIMILARITY_THRESHOLD
                st.session_state.TOP_N = MAX_HISTORY_LEN
                st.session_state.user_query_history = []
                
                # st.rerun()  # í™”ë©´ ì—…ë°ì´íŠ¸

        else:
            # fallback ì·¨ì†Œ
            reply = "â›” ì£„ì†¡í•©ë‹ˆë‹¤ë§Œ ê²€ìƒ‰ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì‹œë©´ ë‹¤ì‹œ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤."
            # st.session_state.chat_messages.append({
            #     "role": "user",
            #     "content": user_input,
            #     "timestamp": current_time
            # })
            st.session_state.chat_messages.append({
                "role": "assistant",
                "content": reply,
                "timestamp": current_time
            })
            # fallback ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.pending_fallback = False
            st.session_state.fallback_attempt = 0
            st.session_state.A_SIMILARITY_THRESHOLD = A_SIMILARITY_THRESHOLD
            st.session_state.TOP_N = MAX_HISTORY_LEN
            st.session_state.user_query_history = []
            st.session_state.embedding_query_text = None
            st.session_state.embedding_query_text_summary = None
            st.session_state.embedding_query_vector = None  # ë²¡í„° ìºì‹± ì´ˆê¸°í™”
            # st.rerun()
        
        if st.session_state.debug_mode:
            pause_here()
        else: 
            st.rerun()

    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    # st.session_state.chat_messages.append({"role": "user", "content": user_input, "timestamp": current_time})
    
    # ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€ ëª…ë ¹ ì²˜ë¦¬
    if user_input.strip().lower() == "ë””ë²„ê·¸":
        st.session_state.debug_mode = not st.session_state.debug_mode
        mode_status = "í™œì„±í™”" if st.session_state.debug_mode else "ë¹„í™œì„±í™”"
        st.session_state.chat_messages.append({
            "role": "assistant", 
            "content": f"ğŸ› ï¸ ë””ë²„ê·¸ ëª¨ë“œê°€ {mode_status}ë˜ì—ˆìŠµë‹ˆë‹¤.", 
            "timestamp": current_time
        })
        debug_info(f"ğŸ¤– ì•—! ë¹„ë°€ì´ í•´ì œë˜ì—ˆìŠµë‹ˆë‹¤. ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”.", pin=True)
        st.rerun()
    
    # ì´ˆê¸°í™” ëª…ë ¹ ì²˜ë¦¬
    elif user_input.strip().lower() == "ì´ˆê¸°í™”":
    
        st.session_state.is_processing = False
        st.session_state.pending_fallback = False
        st.session_state.fallback_attempt = 0
        st.session_state.A_SIMILARITY_THRESHOLD = A_SIMILARITY_THRESHOLD  # ê¸°ë³¸ê°’ ì‚¬ìš©
        st.session_state.TOP_N = MAX_HISTORY_LEN
        st.session_state.embedding_cache = {}
        st.session_state.followup_cache = {}        
        st.session_state.embedding_query_text = None
        st.session_state.embedding_query_text_summary = None
        st.session_state.embedding_query_vector = None  # ë²¡í„° ìºì‹± ì´ˆê¸°í™”
        st.session_state.excluded_keys.clear()
        st.session_state.all_results.clear()
        st.session_state.last_results = []
        st.session_state.user_query_history = []
        st.session_state.conversation_history.clear()
        st.session_state.conversation_history.append({
            "role": "system", 
            "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ AIì…ë‹ˆë‹¤."
        })

        # ì´ˆê¸°í™” ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_messages = []
        st.session_state.chat_messages.append({
            "role": "assistant", 
            "content": "ğŸ¤– ì ì‹œ ë¨¸ë¦¬ ì¢€ ë¹„ìš°ê³  ë‹¤ì‹œ ëŒì•„ì™”ìŠµë‹ˆë‹¤.", 
            "timestamp": current_time
        })
        debug_info(f"ğŸ¤– ì~ ì´ì œ ë‹¤ì‹œ ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.", pin=True)
        st.rerun()
    
    # 'ìì„¸íˆ' ëª…ë ¹ ì²˜ë¦¬
    elif user_input.strip().startswith("ìì„¸íˆ"):
        keyword = user_input.replace("ìì„¸íˆ", "").strip()
        all_stored_results = list(itertools.chain.from_iterable(st.session_state.all_results))
        
        if not all_stored_results:
            reply = "âš ï¸ ì €ì¥ëœ ì¶”ì²œ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤."
        else:
            keywords = keyword.split()
            matches = [
                s for s in all_stored_results
                if all(any(k in s.get(field, "") for field in ["ê¸°ì—…ëª…", "ì„œë¹„ìŠ¤ëª…"]) for k in keywords)
            ]
            
            if not matches:
                reply = "âš ï¸ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê¸°ì—…ëª…ì´ ì—†ìŠµë‹ˆë‹¤."
            elif len(matches) > 1:
                reply = "âš ï¸ ì—¬ëŸ¬ ê°œì˜ ìœ ì‚¬ í•­ëª©ì´ ì¼ì¹˜í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.\n" + "\n".join([f"- {s['ê¸°ì—…ëª…']} : {s['ì„œë¹„ìŠ¤ëª…']}" for s in matches])
            else:
                s = matches[0]
                service_link = f"https://www.tourvoucher.or.kr/user/svcManage/svc/BD_selectSvc.do?svcNo={s['ì„œë¹„ìŠ¤ë²ˆí˜¸']}"
                company_link = f"https://www.tourvoucher.or.kr/user/entrprsManage/provdEntrprs/BD_selectProvdEntrprs.do?entrprsId={s['ê¸°ì—…ID']}"
                
                # ìƒì„¸ ì •ë³´ í˜•ì‹í™”
                details = []
                for k, v in s.items():
                    # ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ: ìˆ«ìë¥¼ ì •ìˆ˜, ì½¤ë§ˆ êµ¬ë¶„ í›„ "ì›" ì¶”ê°€
                    if k == "ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ":
                        try:
                            num = float(v)
                            v = format(round(num), ",") + "ì›"
                        except Exception:
                            pass
                    # ê¸°ì—… ì¸ë ¥í˜„í™©: ì •ìˆ˜ë¡œ í‘œê¸° í›„ "ëª…" ì¶”ê°€
                    elif k == "ê¸°ì—… ì¸ë ¥í˜„í™©":
                        try:
                            num = float(v)
                            v = f"{int(num)}ëª…"
                        except Exception:
                            pass
                    # ê¸°ì—… í•µì‹¬ì—­ëŸ‰: _x000D_ ì œê±°
                    elif k == "ê¸°ì—… í•µì‹¬ì—­ëŸ‰":
                        try:
                            v = v.replace("_x000D_", "")
                        except Exception:
                            pass
                    details.append(f"â€¢&nbsp;{k}: {v}")
                
                # ë§í¬ ë²„íŠ¼ í˜•ì‹ìœ¼ë¡œ ë³€ê²½
                links = [
                    f'<a href="{service_link}" target="_blank" class="link-button">ğŸ”— ì„œë¹„ìŠ¤ ìƒì„¸ ë³´ê¸°</a>',
                    f'<a href="{company_link}" target="_blank" class="link-button">ğŸ¢ ê¸°ì—… ì •ë³´ ë³´ê¸°</a>'
                ]
                
                reply = "ğŸ“„ ì„œë¹„ìŠ¤ ìƒì„¸ì •ë³´\n" + "\n".join(details) + "\n\n" + "\n".join(links)
        
        # ìƒì„¸ ì •ë³´ ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_messages.append({
            "role": "assistant", 
            "content": reply, 
            "timestamp": current_time
        })
        debug_info(f"ğŸ¤– ë§í¬ë¥¼ ëˆ„ë¥´ì‹œë©´ ê´€ê´‘ê³µì‚¬ í™ˆí˜ì´ì§€ ê¸°ì—… ë° ì„œë¹„ìŠ¤ í™”ë©´ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤.", pin=True)
        st.rerun()
    
    # ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬
    else:
    
        # ëŒ€í™” ì´ë ¥ì— ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        st.session_state.conversation_history.append({"role": "user", "content": user_input})
        
        # ì§€ê¸ˆ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ì—†ì–´.
        if not is_relevant_question(user_input):
            
             # ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì—†ê³ , ìµœì´ˆ ëŒ€í™”ê°€ ì•„ë‹Œ ê²½ìš° ê²½ìš°ì„. ë‹¨, ì§€ê¸ˆí•œ ì§ˆë¬¸ì€ ì‚¬ì—…ê³¼ ê´€ë ¨ì„±ì´ ì—†ì–´ë„ ì´ì „ ëŒ€í™”ì™€ì˜ ì—°ê³„ì„±ì„ ê²€í† 
            if st.session_state.user_query_history:
                previous_input = st.session_state.user_query_history[-1]
                
                # ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì—†ê³ , ì´ì „ê³¼ ì§€ê¸ˆì´ ì„œë¡œ ì§„ì§œ ê´€ë ¨ì—†ëŠ” ìƒí™©ì„ --> ì—ëŸ¬!
                if not is_followup_question(previous_input, user_input):
                    debug_info("ğŸ“š 1. ì§€ê¸ˆ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ì—†ì–´. ë”êµ¬ë‚˜ ì´ì „í•œ ì–˜ê¸°ì™€ë„ ì—°ê³„ê°€ ì—†ì–´.")
                    st.session_state.user_query_history.append(user_input)
                    reply = "âš ï¸ ì£„ì†¡í•˜ì§€ë§Œ, ì§ˆë¬¸ì˜ ë‚´ìš©ì„ ì¡°ê¸ˆ ë” ê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ëœ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ í•´ ì£¼ì„¸ìš”."
                    st.session_state.chat_messages.append({
                        "role": "assistant", 
                        "content": reply, 
                        "timestamp": current_time
                    })
                    st.rerun()
                # ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì—†ìœ¼ë‚˜, ì´ì „ê³¼ í›„ì† ëŒ€í™”ì¸ ê²½ìš°ì„. --> í›„ì† ëŒ€í™”ë¡œ ì¸ì§€ --> ë‹¨, ê²€ìƒ‰ ì‹œ ë‚´ìš©ì€ ì´ì „ê±¸ë¡œ ë„£ì–´ì¤˜ì•¼ í•´.
                # ex. ì´ì „ : í™ˆí˜ì´ì§€ êµ¬ì¶• ì—…ì²´ ì•Œë ¤ì¤˜.
                #     ì§€ê¸ˆ : ë‹¤ë¥¸ ì‚¬ë¡€ëŠ” ì—†ì–´? 
                else:
                    debug_info("ğŸ“š 2. ì§€ê¸ˆ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ì—†ì–´. í•˜ì§€ë§Œ ì§€ê¸ˆ ì–˜ê¸°í•œê±´ ì´ì „ì— ì–˜ê¸°ì™€ëŠ” ì—°ê´€ë˜ì–´ ìˆì–´.")
                    st.session_state.embedding_query_text = "[ì´ì „ ì§ˆë¬¸ : ]" + previous_input + "\n[ì§€ê¸ˆ ì§ˆë¬¸ : ]" + user_input
                    
            # ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì—†ê³ , ìµœì´ˆ ëŒ€í™”ì¸ ê²½ìš°ì„ ë˜ëŠ” Fallback í›„ ì´ˆê¸°í™” ëœ ì´í›„ì„. --> ì—ëŸ¬!
            else: 
                debug_info("ğŸ“š 3. ì§€ê¸ˆ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ì—†ì–´. í•˜ì§€ë§Œ ìµœì´ˆë¶€í„° ì´ëŸ° ê´€ë ¨ì—†ëŠ” ì–˜ê¸°í•˜ë©´ ì•ˆë˜ëŠ”ê±°ì•¼.")
                st.session_state.user_query_history.append(user_input)
                reply = "âš ï¸ ì£„ì†¡í•˜ì§€ë§Œ, ì§ˆë¬¸ì˜ ë‚´ìš©ì„ ì¡°ê¸ˆ ë” ê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ëœ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ í•´ ì£¼ì„¸ìš”."
                st.session_state.chat_messages.append({
                    "role": "assistant", 
                    "content": reply, 
                    "timestamp": current_time
                })
                st.rerun()
                
        # ì§€ê¸ˆí•œ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ìˆìŒ.
        else:
            
            # ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ìˆê³ , ìµœì´ˆ ëŒ€í™”ê°€ ì•„ë‹Œ ê²½ìš°ì„. ë‹¤ë§Œ ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì´ì „ê³¼ ê´€ë ¨ì„±ì„ ê²€í† í•œë‹¤.
            if st.session_state.user_query_history:
                
                previous_input = st.session_state.user_query_history[-1]
                # ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ ìˆìœ¼ë‚˜, ì´ì „ê³¼ ì§€ê¸ˆì´ ì„œë¡œ ì—°ì†ì„±ì€ ì—†ëŠ” ê²½ìš°. --> ì‹ ê·œ ëŒ€í™”ë¡œ ì „í™˜     
                # ex. ì´ì „ : í™ˆí˜ì´ì§€ êµ¬ì¶• ì—…ì²´ ì•Œë ¤ì¤˜.
                #     ì§€ê¸ˆ : ì•„ë‹ˆì•¼. ë””ìì¸ í™ë³´ ì—…ì²´ë¥¼ ìƒˆë¡œ ì•Œë ¤ì¤˜. 
                if not is_followup_question(previous_input, user_input):
                    debug_info("ğŸ“š 4. ì§€ê¸ˆ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì‚¬ì—…ê³¼ëŠ” ê´€ë ¨ìˆì§€ë§Œ, ì•ì—ì„œ ì–˜ê¸°í•œ ì‚¬ì—…ì´ë‘ì€ ì „í˜€ ê´€ë ¨ì´ ì—†ì–´. í•˜ì§€ë§Œ ìƒˆë¡­ê²Œ ë‹¤ë¥¸ ì‚¬ì—…ê³¼ ê´€ë ¨ìˆëŠ” ì–˜ê¸°í•˜ë©´ ì¢‹ì€ê±°ì•¼.")
                    # st.session_state.embedding_query_text = user_input
                    st.session_state.embedding_query_text = "[ì´ì „ ì§ˆë¬¸ : ]" + previous_input + "\n[ì§€ê¸ˆ ì§ˆë¬¸ : ]" + user_input
                    
                # ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ ìˆê³ , ì´ì „ì˜ ëŒ€í™”ì™€ê³  ê´€ë ¨ì´ ìˆìŒ.   --> í›„ì† ëŒ€í™”ë¡œ ì¸ì§€    
                # ex. ì´ì „ : í™ˆí˜ì´ì§€ êµ¬ì¶• ì—…ì²´ ì•Œë ¤ì¤˜.
                #     ì§€ê¸ˆ : í™ˆí˜ì´ì§€ êµ¬ì¶• ì—…ì²´ë¥¼ ì¶”ê°€ë¡œ ì•Œë ¤ì¤˜.
                else:
                    debug_info("ğŸ“š 5. ì§€ê¸ˆ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì‚¬ì—…ê³¼ëŠ” ê´€ë ¨ë„ ìˆê³ , ì•ì—ì„œ ì–˜ê¸°í•œ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ìˆì–´. ê·¸ë¦¬ê³  ì§€ê¸ˆ ì–˜ê¸°í•œ ê²ƒë„ êµ¬ì²´ì ìœ¼ë¡œ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ë˜ì–´ ìˆì–´.")
                    st.session_state.embedding_query_text = "[ì´ì „ ì§ˆë¬¸ : ]" + previous_input + "\n[ì§€ê¸ˆ ì§ˆë¬¸ : ]" + user_input
                    
            # ì§€ê¸ˆí•œ ì§ˆë¬¸ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ìˆê³ , ìµœì´ˆ ëŒ€í™”í•œ ê²½ìš° ë˜ëŠ” Fallback í›„ ì´ˆê¸°í™” ëœ ì´í›„ì„. --> ì‹ ê·œ ëŒ€í™”ë¡œ ì¸ì§€
            else:
                debug_info("ğŸ“š 6. ì§€ê¸ˆ ì§ˆë¬¸í•œ ë‚´ìš©ì´ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ìˆì–´. ê·¸ë¦¬ê³  ì§€ê¸ˆ ìµœì´ˆë¡œ ì–˜ê¸°í•œ ê²ƒë„ ì‚¬ì—…ê³¼ ê´€ë ¨ì´ ë˜ì–´ ìˆì–´.")
                st.session_state.embedding_query_text = user_input
                
    
        
    
    
        # # í›„ì† ì§ˆë¬¸ íŒë‹¨
        # if st.session_state.user_query_history:
        #     previous_input = st.session_state.user_query_history[-1]
        #     if not is_followup_question(previous_input, user_input):
        #         debug_info("ğŸ¤– ì‹ ê·œ ì§ˆë¬¸ìœ¼ë¡œ ì¸ì‹í•˜ê³  ê´€ë ¨ ì„œë¹„ìŠ¤ë¥¼ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤...", pin=False)
        #         st.session_state.embedding_query_text = user_input
        #     else:
        #         # í›„ì† ì§ˆë¬¸ì´ë©´ ì´ì „ ì„ë² ë”© ìœ ì§€
        #         debug_info("ğŸ¤– í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ì¸ì‹í•˜ê³  ê´€ë ¨ ì„œë¹„ìŠ¤ë¥¼ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤...", pin=False)
        # else:    
        #     # ìµœì´ˆ ì§ˆë¬¸ì¸ ê²½ìš°
        #     debug_info("ğŸ¤– ìµœì´ˆ ì§ˆë¬¸ìœ¼ë¡œ ì¸ì‹í•˜ê³  ê´€ë ¨ ì„œë¹„ìŠ¤ë¥¼ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤...", pin=False)
        #     st.session_state.embedding_query_text = user_input
        
        
        
        # # ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        st.session_state.user_query_history.append(user_input)
        debug_info("ğŸ“š embedding_query_text = " + st.session_state.embedding_query_text, pin=False)
        
        debug_info("ğŸ¤– ê´€ë ¨ ì„œë¹„ìŠ¤ë¥¼ ì°¾ëŠ” ì¤‘ì…ë‹ˆë‹¤...", pin=False)
        # ì¶”ì²œ ëª¨ë“œ ì„¤ì • ë° ì„œë¹„ìŠ¤ ì¶”ì²œ
        best_mode = is_best_recommendation_query(user_input)
        exclude = None if best_mode else st.session_state.excluded_keys
        last_results = recommend_services(
            st.session_state.embedding_query_text, 
            exclude_keys=exclude, 
            use_random=not best_mode
        )
        
        # ì¶”ì²œ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°
        if not last_results:
            st.session_state.pending_fallback = True
            reply = "âš ï¸ ì§ˆë¬¸ ì˜ë„ íŒŒì•…ì´ ì‰½ì§€ ì•Šê±°ë‚˜ ì›í•˜ì‹œëŠ” ì¶”ì²œ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ê´€ë ¨ëœ ì—…ì²´ë‚˜ ì„œë¹„ìŠ¤ë¥¼ ì œê³µë“œë¦¬ê¸°ê°€ ì–´ë µìŠ¤ëŸ½ìŠµë‹ˆë‹¤. í•œ ë²ˆ ë” ì§„í–‰ì„ ì›í•˜ì‹œë©´ 'ë„¤' ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”."
            st.session_state.chat_messages.append({
                "role": "assistant", 
                "content": reply, 
                "timestamp": current_time
            })
            debug_info(f"ğŸ¤– ...", pin=True)
            if st.session_state.debug_mode:
                pause_here()
            else: 
                st.rerun()
        
        debug_info("ğŸ¤– ì¶”ì²œ ë‚´ìš©ì„ ì •ë¦¬ ì¤‘ì…ë‹ˆë‹¤...", pin=False)
        # ì¶”ì²œ ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        unique_last_results = [
            s for s in last_results
            if (s["ê¸°ì—…ID"], s.get("ì„œë¹„ìŠ¤ìœ í˜•"), s.get("ì„œë¹„ìŠ¤ëª…")) not in st.session_state.excluded_keys
        ]
        context = make_context(unique_last_results)
        gpt_prompt = make_prompt(user_input, context, is_best=best_mode)
        
        # GPTì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬
        st.session_state.conversation_history.append({"role": "user", "content": gpt_prompt})
        
        try:
            gpt_reply = ask_gpt(list(st.session_state.conversation_history))
        except Exception as e:
            gpt_reply = f"âš ï¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”: {str(e)}"
            
        # ì‘ë‹µì„ ì €ì¥í•˜ê³  ëŒ€í™” ì´ë ¥ì— ì¶”ê°€
        st.session_state.conversation_history.append({"role": "assistant", "content": gpt_reply})
        
        # ì´ë²ˆ ì¶”ì²œì—ì„œ ì–¸ê¸‰ëœ ì„œë¹„ìŠ¤ë“¤ ì œì™¸ ëŒ€ìƒìœ¼ë¡œ ë“±ë¡
        mentioned_keys = {
            (s["ê¸°ì—…ID"], s.get("ì„œë¹„ìŠ¤ìœ í˜•"), s.get("ì„œë¹„ìŠ¤ëª…"))
            for s in last_results
            if (
                str(s["ê¸°ì—…ID"]) in gpt_reply and
                s["ì„œë¹„ìŠ¤ëª…"] in gpt_reply
            )
        }
        
        debug_info(f"[â—DEBUG] GPT ì‘ë‹µì—ì„œ ì–¸ê¸‰ëœ í‚¤: {mentioned_keys}")
        for s in last_results:
            ê¸°ì—…ID = s.get('ê¸°ì—…ID')
            ê¸°ì—…ëª… = s.get('ê¸°ì—…ëª…')
            ì„œë¹„ìŠ¤ëª… = s.get('ì„œë¹„ìŠ¤ëª…')
            debug_info(f"ğŸ” ë¹„êµ ì¤‘: {ê¸°ì—…ID} / {ê¸°ì—…ëª…} / {ì„œë¹„ìŠ¤ëª…}")
            debug_info(f"    â†’ ê¸°ì—…ID ë¹„êµ: {ê¸°ì—…ID} in GPT? {'YES' if str(ê¸°ì—…ID) in gpt_reply else 'NO'}")
            debug_info(f"    â†’ ê¸°ì—…ëª… ë¹„êµ: {ê¸°ì—…ëª…} in GPT? {'YES' if ê¸°ì—…ëª… in gpt_reply else 'NO'}")
            debug_info(f"    â†’ ì„œë¹„ìŠ¤ëª… ë¹„êµ: {ì„œë¹„ìŠ¤ëª…} in GPT? {'YES' if ì„œë¹„ìŠ¤ëª… in gpt_reply else 'NO'}")
        debug_info(f"\nğŸ” excluded_keys í‚¤ ìˆ˜: {len(st.session_state.excluded_keys)}")
        
        # ì œì™¸ ëŒ€ìƒ ì—…ë°ì´íŠ¸
        st.session_state.excluded_keys.update(mentioned_keys)
        
        # ì¶”ì²œ ê²°ê³¼ ì €ì¥
        st.session_state.last_results = last_results
        st.session_state.all_results.append(last_results)
        
        # ì±—ë´‡ ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_messages.append({
            "role": "assistant", 
            "content": gpt_reply, 
            "timestamp": current_time
        })
                
        if st.session_state.debug_mode:
            if "embedding_query_text" in st.session_state:
                debug_info("ğŸ“š embedding_query_text =\n" + st.session_state.embedding_query_text)
            if "unique_last_results" in st.session_state:
                debug_info("ğŸ“š unique_last_results = " + json.dumps(st.session_state.unique_last_results, ensure_ascii=False, indent=2))
            if "context" in locals():
                debug_info("ğŸ“š context =\n" + context)
            if "gpt_prompt" in locals():
                debug_info("ğŸ“š gpt_prompt =\n" + gpt_prompt)
            if "gpt_reply" in locals():
                debug_info("ğŸ“š gpt_reply =\n" + gpt_reply)
            if "conversation_history" in st.session_state:
                debug_info("ğŸ“š conversation_history = " + json.dumps(list(st.session_state.conversation_history), ensure_ascii=False, indent=2))
            if "last_results" in st.session_state:
                debug_info("ğŸ“š last_results = " + json.dumps(st.session_state.last_results, ensure_ascii=False, indent=2))
            if "all_results" in st.session_state:
                debug_info("ğŸ“š all_results = " + json.dumps(list(st.session_state.all_results), ensure_ascii=False, indent=2))
            pause_here()
        else:    
            st.rerun()
        