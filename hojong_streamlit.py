# hojong_streamlit.py
import streamlit as st
import faiss
import pickle
import numpy as np
import random
import itertools
from collections import deque
from openai import OpenAI

# OpenAI client ì´ˆê¸°í™”
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
index = faiss.read_index("service_index.faiss")
with open("service_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

# ë²¡í„° ì •ê·œí™” í•¨ìˆ˜
def normalize(vecs):
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    return vecs / norms

# ë²¡í„° ì¬êµ¬ì„± ë° cosine similarityìš© ì¸ë±ìŠ¤ ì¤€ë¹„
xb = index.reconstruct_n(0, index.ntotal)
xb = normalize(xb)
d = xb.shape[1]
index_cosine = faiss.IndexFlatIP(d)
index_cosine.add(xb)

SIMILARITY_THRESHOLD = 0.30

# ----------------------- ì„ë² ë”© í•¨ìˆ˜ ----------------------- #
def get_embedding(text, model="text-embedding-3-small"):
    response = client.embeddings.create(input=[text], model=model)
    return response.data[0].embedding

# ----------------------- GPT í•¨ìˆ˜ ----------------------- #
def ask_gpt(messages):
    response = client.chat.completions.create(model="gpt-4o", messages=messages)
    return response.choices[0].message.content

# ----------------------- Streamlit ìƒíƒœ ì´ˆê¸°í™” ----------------------- #
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤."}
    ]
if "all_results" not in st.session_state:
    st.session_state.all_results = deque(maxlen=5)
if "excluded_company_ids" not in st.session_state:
    st.session_state.excluded_company_ids = set()
if "last_results" not in st.session_state:
    st.session_state.last_results = []
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []

# ----------------------- ì¶”ì²œ ê´€ë ¨ í•¨ìˆ˜ ----------------------- #
def is_best_recommendation_query(query):
    keywords = ["ê°€ì¥", "ìµœê³ ", "ì œì¼", "1ë“±", "1ìœ„", "ì§„ì§œ ì¶”ì²œ", "ê°•ë ¥ ì¶”ì²œ", "ì •ë§ ì¶”ì²œ", "ìµœì„ "]
    return any(k in query for k in keywords)

def recommend_services(query, top_k=5, exclude_company_ids=None):
    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec = normalize(query_vec)
    D, indices = index_cosine.search(query_vec, 100)

    candidate_services = {}
    for score, idx in zip(D[0], indices[0]):
        service = metadata[idx]
        cid = service["ê¸°ì—…ID"]
        if exclude_company_ids and cid in exclude_company_ids:
            continue
        service_type = service.get("ì„œë¹„ìŠ¤ìœ í˜•", None)
        key = (cid, service_type)
        candidate_services.setdefault(key, []).append((score, service))

    best_services = []
    for candidate_list in candidate_services.values():
        chosen_score, chosen_service = random.choice(candidate_list)
        best_services.append((chosen_score, chosen_service))
    best_services.sort(key=lambda x: x[0], reverse=True)
    return [s for _, s in best_services[:top_k]]

def is_relevant_question(query, threshold=SIMILARITY_THRESHOLD):
    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec = normalize(query_vec)
    D, _ = index_cosine.search(query_vec, 1)
    return D[0][0] >= threshold

# ----------------------- GPT Prompt êµ¬ì„± ----------------------- #
def make_context(results):
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n"
        f"- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}\n"
        f"- ê¸ˆì•¡: {s.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')} / ê¸°í•œ: {s.get('ì„œë¹„ìŠ¤ê¸°í•œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ë²•ì¸ì—¬ë¶€: {s.get('ê¸°ì—…ì˜ ë²•ì¸ì—¬ë¶€', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìœ„ì¹˜: {s.get('ê¸°ì—… ìœ„ì¹˜', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•µì‹¬ì—­ëŸ‰: {s.get('ê¸°ì—… í•µì‹¬ì—­ëŸ‰', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- 3ê°œë…„ í‰ê·  ë§¤ì¶œ: {s.get('ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•´ë‹¹ë¶„ì•¼ì—…ë ¥: {s.get('ê¸°ì—… í•´ë‹¹ë¶„ì•¼ì—…ë ¥', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì£¼ìš”ì‚¬ì—…ë‚´ìš©: {s.get('ê¸°ì—… ì£¼ìš”ì‚¬ì—…ë‚´ìš©', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì¸ë ¥í˜„í™©: {s.get('ê¸°ì—… ì¸ë ¥í˜„í™©', 'ì •ë³´ ì—†ìŒ')}"
        for i, s in enumerate(results)
    ])

def make_summary_context(summary_memory):
    seen = set()
    deduplicated = []
    for result_list in reversed(summary_memory):
        for item in result_list:
            key = (item['ì„œë¹„ìŠ¤ëª…'], item['ê¸°ì—…ëª…'], item.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì—†ìŒ'))
            if key not in seen:
                seen.add(key)
                deduplicated.insert(0, item)
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}"
        for i, s in enumerate(deduplicated)
    ])

def make_prompt(query, context, is_best=False):
    if "ì¶”ì²œ" in query:
        style_instruction = (
            "- ë‹µë³€ì€ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”. ê° ì¶”ì²œ í•­ëª©ì€ ë²ˆí˜¸ë¥¼ ë¶™ì´ê³ , ê¸°ì—…ëª…, ì„œë¹„ìŠ¤ëª…, ì„œë¹„ìŠ¤ ìœ í˜•, ê¸ˆì•¡, ê¸°í•œ, ë²•ì¸ì—¬ë¶€, ìœ„ì¹˜, í•µì‹¬ì—­ëŸ‰, 3ê°œë…„ í‰ê·  ë§¤ì¶œ, í•´ë‹¹ë¶„ì•¼ì—…ë ¥, ì£¼ìš”ì‚¬ì—…ë‚´ìš©, ì¸ë ¥í˜„í™©ì„ ìƒì„¸í•˜ê²Œ ê¸°ìˆ í•´ ì£¼ì„¸ìš”.\n"
            "- ì„œë¹„ìŠ¤ëª…ê³¼ ê¸°ì—…ëª…ì€ ë°˜ë“œì‹œ ë”°ì˜´í‘œë¡œ ë¬¶ê³ , ëª©ë¡ì€ ëŒ€ì‹œ(-)ë¡œ ë‚˜ì—´í•´ ì£¼ì„¸ìš”."
        )
    else:
        style_instruction = "- ë‹µë³€ì€ ì„œìˆ ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. ê¸°ì—… ì •ë³´ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”."

    extra = ""
    if is_best:
        history = make_summary_context(st.session_state.all_results)
        extra = f"\nì§€ê¸ˆê¹Œì§€ ì¶”ì²œí•œ ì„œë¹„ìŠ¤ ëª©ë¡:\n{history}\nì´ì „ì— ì¶”ì²œëœ ê¸°ì—…ë„ í¬í•¨í•´ì„œ ìµœê³ ì˜ ì¡°í•©ì„ ì œì‹œí•´ ì£¼ì„¸ìš”."

    return f"""
ë‹¹ì‹ ì€ ê´€ê´‘ìˆ˜í˜œê¸°ì—…ì—ê²Œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” AI ìƒë‹´ì‚¬ í˜¸ì¢…ì´ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{query}"

ê´€ë ¨ ì„œë¹„ìŠ¤ ëª©ë¡:
{context}

ğŸ“Œ ì¡°ê±´:
{style_instruction}
{extra}
- ë™ì¼í•œ ê¸°ì—…ì˜ ì„œë¹„ìŠ¤ê°€ ì—¬ëŸ¬ ê°œì¼ ê²½ìš°, í•˜ë‚˜ë§Œ ì„ íƒí•´ ì£¼ì„¸ìš”.
- íŠ¹ìˆ˜ë¬¸ìëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , ë¶€ë“œëŸ¬ìš´ ìƒë‹´ì‚¬ ë§íˆ¬ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
"""

# ----------------------- Streamlit UI ----------------------- #
st.markdown("<h1 style='text-align: center;'>ê´€ê´‘ê³µì‚¬ ì„œë¹„ìŠ¤ ê°€ì´ë“œ AI</h1>", unsafe_allow_html=True)
st.markdown("<p style='text-align: center; font-size:14px;'>ğŸ¤– í˜¸ì¢…ì´ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”.</p>", unsafe_allow_html=True)

for msg in st.session_state.chat_messages:
    if msg["role"] == "user":
        st.markdown(f"<p style='background-color:#DCF8C6; padding:8px; border-radius:5px; text-align:right;'>{msg['content']}</p>", unsafe_allow_html=True)
    else:
        st.markdown(f"<p style='background-color:#FFFFFF; padding:8px; border-radius:5px; text-align:left;'>{msg['content']}</p>", unsafe_allow_html=True)

st.markdown("<p style='text-align:center; font-size:12px;'>\"ìì„¸íˆ ê¸°ì—…ëª…\" ì„ ì…ë ¥í•˜ì‹œë©´ ë³´ë‹¤ ìƒì„¸í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</p>", unsafe_allow_html=True)

with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_area("ë©”ì‹œì§€ ì…ë ¥", height=80)
    submitted = st.form_submit_button("ë¬¼ì–´ë³´ê¸°")

if submitted and user_input.strip():
    st.session_state.conversation_history.append({"role": "user", "content": user_input})
    st.session_state.chat_messages.append({"role": "user", "content": user_input})

    if user_input.startswith("ìì„¸íˆ"):
        keyword = user_input.replace("ìì„¸íˆ", "").strip()
        all_results = list(itertools.chain.from_iterable(st.session_state.all_results))
        matches = [s for s in all_results if keyword in s["ê¸°ì—…ëª…"]]
        if not matches:
            reply = "í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê¸°ì—…ëª…ì´ ì—†ìŠµë‹ˆë‹¤."
        elif len(matches) > 1:
            reply = "ì—¬ëŸ¬ ê°œì˜ ê¸°ì—…ëª…ì´ ì¼ì¹˜í•©ë‹ˆë‹¤:\n" + "\n".join(f"- {s['ê¸°ì—…ëª…']}" for s in matches)
        else:
            s = matches[0]
            service_link = f"https://www.tourvoucher.or.kr/user/svcManage/svc/BD_selectSvc.do?svcNo={s['ì„œë¹„ìŠ¤ë²ˆí˜¸']}"
            company_link = f"https://www.tourvoucher.or.kr/user/entrprsManage/provdEntrprs/BD_selectProvdEntrprs.do?entrprsId={s['ê¸°ì—…ID']}"
            details = []
            for k, v in s.items():
                if k == "ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ":
                    try: v = format(int(float(v)), ",") + "ì›"
                    except: pass
                elif k == "ê¸°ì—… ì¸ë ¥í˜„í™©":
                    try: v = f"{int(float(v))}ëª…"
                    except: pass
                elif k == "ê¸°ì—… í•µì‹¬ì—­ëŸ‰":
                    v = v.replace("_x000D_", "")
                details.append(f"{k}: {v}")
            reply = "\n".join(details) + f"\nğŸ”— ì„œë¹„ìŠ¤ ë§í¬: {service_link}\nğŸ¢ ê¸°ì—… ë§í¬: {company_link}"
        st.session_state.chat_messages.append({"role": "assistant", "content": reply})
    else:
        if not is_relevant_question(user_input):
            msg = "ì£„ì†¡í•˜ì§€ë§Œ, ê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."
            st.session_state.chat_messages.append({"role": "assistant", "content": msg})
        else:
            best_mode = is_best_recommendation_query(user_input)
            exclude = None if best_mode else st.session_state.excluded_company_ids
            results = recommend_services(user_input, exclude_company_ids=exclude)
            if not best_mode:
                for s in results:
                    st.session_state.excluded_company_ids.add(s["ê¸°ì—…ID"])
            st.session_state.last_results = results
            st.session_state.all_results.append(results)
            context = make_context(results)
            prompt = make_prompt(user_input, context, is_best=best_mode)
            st.session_state.conversation_history.append({"role": "user", "content": prompt})
            gpt_reply = ask_gpt(st.session_state.conversation_history)
            gpt_reply = gpt_reply.replace("\n\n", "\n")
            st.session_state.conversation_history.append({"role": "assistant", "content": gpt_reply})
            st.session_state.chat_messages.append({"role": "assistant", "content": gpt_reply})

    st.experimental_rerun()
