# âœ… Streamlit ê¸°ë°˜ ê´€ê´‘ê¸°ì—… ì¶”ì²œ ì±—ë´‡ (ì›ë³¸ í”„ë¡¬í”„íŠ¸ ë° ë¬¸êµ¬ ì™„ì „ ë³´ì¡´)

import streamlit as st
import faiss
import pickle
import numpy as np
import random
import itertools
from collections import deque
from openai import OpenAI

# âœ… OpenAI í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# âœ… ê¸°ë³¸ ìŠ¤íƒ€ì¼ ì •ì˜
st.markdown("""
    <style>
        html, body, [class*="css"] {
            background-color: #000 !important;
            color: #fff !important;
            font-family: 'Noto Sans KR', sans-serif;
        }
        .log-message {
            font-size: 13px;
            margin-top: 10px;
            color: #aaa;
        }
    </style>
""", unsafe_allow_html=True)

# âœ… ì „ì—­ ìƒìˆ˜
SIMILARITY_THRESHOLD = 0.30
MAX_HISTORY_LEN = 10

# âœ… ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = deque(maxlen=MAX_HISTORY_LEN + 1)
    st.session_state.conversation_history.append({"role": "system", "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ ì•„ë˜ ì§€ì¹¨ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:\n\n- ë‹µë³€ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ ë§íˆ¬ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n- ì‚¬ìš©ì ì§ˆë¬¸ì— \"ì¶”ì²œ\", \"ì œì‹œ\", \"ì°¾ì•„\", \"ê²€ìƒ‰í•´\" ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ë©´ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ê° í•­ëª©ì— ê¸°ì—…ëª…, ì„œë¹„ìŠ¤ëª…, ê¸°ì—…ID, ì„œë¹„ìŠ¤ìœ í˜•, ê¸ˆì•¡, ê¸°í•œ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”. ë§Œì¼ ê·¸ë ‡ì§€ ì•Šìœ¼ë©°, ì„œìˆ ì‹ì¼ ê²½ìš° ìì—°ìŠ¤ëŸ½ê³  í¬ê´„ì ì¸ ì„¤ëª…ìœ¼ë¡œ êµ¬ì„±í•´ ì£¼ì„¸ìš”.\n- ëª©ë¡ìœ¼ë¡œ ì¶œë ¥í•  ê²½ìš° ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n\n  1. \"ì„œë¹„ìŠ¤ëª…\"\n     - \"ê¸°ì—…ëª…\" (ê¸°ì—…ID: XXXX)\n     - ìœ í˜•: ...\n     - ê¸ˆì•¡: ...\n     - ê¸°í•œ: ...\n     - ìš”ì•½: ...\n     -...\n\n- ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼(**, ## ë“±)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n- ë¶ˆë¦¿ì€ í•­ìƒ ëŒ€ì‹œ(-)ë§Œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.\n- í•­ëª© ê°„ ê°œí–‰ì„ ë„£ì§€ ë§ˆì„¸ìš”.\n- ê¸°ì—…ëª…/ì„œë¹„ìŠ¤ëª…ì´ ì–¸ê¸‰ë˜ë©´ ë°˜ë“œì‹œ ì´ìœ ë„ í•¨ê»˜ ì œì‹œí•´ ì£¼ì„¸ìš”.\n- ê¸°ì—…IDëŠ” ë°˜ë“œì‹œ ê´„í˜¸ ì•ˆì— í‘œê¸°í•´ ì£¼ì„¸ìš”. ì˜ˆ: \"ì œì´ì–´ìŠ¤\" (ê¸°ì—…ID: 12345)"})
if "excluded_keys" not in st.session_state:
    st.session_state.excluded_keys = set()
if "all_results" not in st.session_state:
    st.session_state.all_results = deque(maxlen=MAX_HISTORY_LEN)
if "user_query_history" not in st.session_state:
    st.session_state.user_query_history = []

# âœ… ìœ í‹¸ ë¡œê·¸ ì¶œë ¥
log_messages = []
def log(msg):
    log_messages.append(msg)

# âœ… FAISS index ë¡œë”©
index = faiss.read_index("service_index.faiss")
with open("service_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

xb = index.reconstruct_n(0, index.ntotal)
xb = xb / np.linalg.norm(xb, axis=1, keepdims=True)
d = xb.shape[1]
index_cosine = faiss.IndexFlatIP(d)
index_cosine.add(xb)

# âœ… ì„ë² ë”©
embedding_cache = {}
def get_embedding(text, model="text-embedding-3-small"):
    if text in embedding_cache:
        log(f"[CACHE] ì„ë² ë”© ìºì‹œ ì‚¬ìš©: '{text}'")
        return embedding_cache[text]
    log(f"[EMBED] OpenAI ì„ë² ë”© ìƒì„±: '{text}'")
    res = client.embeddings.create(input=[text], model=model)
    embedding = res.data[0].embedding
    embedding_cache[text] = embedding
    return embedding

# âœ… GPT í˜¸ì¶œ
def ask_gpt(messages):
    res = client.chat.completions.create(model="gpt-4o", messages=messages)
    return res.choices[0].message.content

# âœ… ì„œë¹„ìŠ¤ ëª©ë¡ ìƒì„±

def make_context(results):
    return "\n".join([
        f"{i+1}. \"{s['ì„œë¹„ìŠ¤ëª…']}\" (ì œê³µê¸°ì—…: \"{s['ê¸°ì—…ëª…']}\", ê¸°ì—…ID: {s['ê¸°ì—…ID']})\n"
        f"- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}\n"
        f"- ê¸ˆì•¡: {s.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')} / ê¸°í•œ: {s.get('ì„œë¹„ìŠ¤ê¸°í•œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ë²•ì¸ì—¬ë¶€: {s.get('ê¸°ì—…ì˜ ë²•ì¸ì—¬ë¶€', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìœ„ì¹˜: {s.get('ê¸°ì—… ìœ„ì¹˜', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•µì‹¬ì—­ëŸ‰: {s.get('ê¸°ì—… í•µì‹¬ì—­ëŸ‰', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- 3ê°œë…„ í‰ê·  ë§¤ì¶œ: {s.get('ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•´ë‹¹ë¶„ì•¼ì—…ë ¥: {s.get('ê¸°ì—… í•´ë‹¹ë¶„ì•¼ì—…ë ¥', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì£¼ìš”ì‚¬ì—…ë‚´ìš©: {s.get('ê¸°ì—… ì£¼ìš”ì‚¬ì—…ë‚´ìš©', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì¸ë ¥í˜„í™©: {s.get('ê¸°ì—… ì¸ë ¥í˜„í™©', 'ì •ë³´ ì—†ìŒ')}"
        for i, s in enumerate(results)
    ])

# âœ… ì¶”ì²œ í”„ë¡¬í”„íŠ¸ ìƒì„± (ê¸°ì¡´ ì›ë³¸ ê·¸ëŒ€ë¡œ)
def make_prompt(query, context, is_best=False):
    if is_best:
        history = make_summary_context(st.session_state.all_results)
        extra = f"ì§€ê¸ˆê¹Œì§€ ì¶”ì²œí•œ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{history}\n\nì´ì „ì— ì¶”ì²œëœ ê¸°ì—…ë„ í¬í•¨í•´ì„œ ì¡°ê±´ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ìµœê³ ì˜ ì¡°í•©ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
    else:
        extra = ""

    return f"""ë‹¹ì‹ ì€ ê´€ê´‘ìˆ˜í˜œê¸°ì—…ì—ê²Œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” AI ìƒë‹´ì‚¬ í˜¸ì¢…ì´ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{query}"

ê·¸ë¦¬ê³  ê´€ë ¨ëœ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:
{context}

ğŸ“Œ {extra}
"""

# âœ… ìš”ì•½ ì»¨í…ìŠ¤íŠ¸

def make_summary_context(summary_memory):
    seen = set()
    deduped = []
    for result_list in reversed(summary_memory):
        for item in result_list:
            key = (item['ì„œë¹„ìŠ¤ëª…'], item['ê¸°ì—…ëª…'], item.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', ''))
            if key not in seen:
                seen.add(key)
                deduped.insert(0, item)
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}"
        for i, s in enumerate(deduped)
    ])

# âœ… ì¶”ì²œ ë¡œì§ (ë¡œì»¬ ë¡œì§ ê·¸ëŒ€ë¡œ ìœ ì§€)
def recommend_services(query, top_k=5, exclude_keys=None, use_random=True):
    if exclude_keys is None:
        exclude_keys = set()

    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec = query_vec / np.linalg.norm(query_vec)
    D, indices = index_cosine.search(query_vec, 300)
    ranked = [(score, metadata[idx]) for score, idx in zip(D[0], indices[0])]

    seen_keys = set()
    filtered = []
    for score, service in ranked:
        key = (service["ê¸°ì—…ID"], service.get("ì„œë¹„ìŠ¤ìœ í˜•"), service.get("ì„œë¹„ìŠ¤ëª…"))
        if key in exclude_keys or key in seen_keys:
            continue
        seen_keys.add(key)
        filtered.append((score, service))

    filtered.sort(key=lambda x: x[0], reverse=True)
    if use_random:
        top_10 = filtered[:10]
        selected = random.sample(top_10, min(len(top_10), top_k))
        return [s for _, s in selected]
    return [s for _, s in filtered[:top_k]]

# âœ… ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
st.title("ğŸ¯ ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ ì¶”ì²œ ì±—ë´‡ (í˜¸ì¢…ì´)")
st.markdown("""<small>ê´€ê´‘ ìˆ˜í˜œê¸°ì—…ì„ ìœ„í•œ ë§ì¶¤í˜• ì¶”ì²œì„ ë„ì™€ë“œë ¤ìš”.</small>""", unsafe_allow_html=True)

with st.form("chat_form", clear_on_submit=True):
    user_input = st.text_area("í˜¸ì¢…ì´ì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš”", height=100)
    submitted = st.form_submit_button("ë¬¼ì–´ë³´ê¸°")

if submitted and user_input:
    best_mode = any(k in user_input for k in ["ê°•ë ¥ ì¶”ì²œ", "ê°•ì¶”"])
    exclude = None if best_mode else st.session_state.excluded_keys

    st.session_state.user_query_history.append(user_input)
    results = recommend_services(user_input, exclude_keys=exclude, use_random=not best_mode)

    st.session_state.all_results.append(results)
    context = make_context(results)
    gpt_prompt = make_prompt(user_input, context, is_best=best_mode)

    st.session_state.conversation_history.append({"role": "user", "content": gpt_prompt})
    try:
        reply = ask_gpt(list(st.session_state.conversation_history))
    except Exception as e:
        reply = f"â— GPT ì‘ë‹µ ì‹¤íŒ¨: {e}"

    st.session_state.conversation_history.append({"role": "assistant", "content": reply})

    # GPT ì‘ë‹µì—ì„œ ì–¸ê¸‰ëœ í•­ëª© í•„í„°ë§
    mentioned_keys = {
        (s["ê¸°ì—…ID"], s.get("ì„œë¹„ìŠ¤ìœ í˜•"), s.get("ì„œë¹„ìŠ¤ëª…"))
        for s in results
        if (s["ê¸°ì—…ID"] in reply and s["ì„œë¹„ìŠ¤ëª…"] in reply)
    }
    st.session_state.excluded_keys.update(mentioned_keys)

    # ì¶œë ¥
    st.markdown("---")
    st.markdown("<b>ğŸ¤– í˜¸ì¢…ì´ ì¶”ì²œ:</b>", unsafe_allow_html=True)
    st.markdown(reply.replace("\n", "  "), unsafe_allow_html=True)

# âœ… ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥
if log_messages:
    st.markdown("""<div class='log-message'>""" + "<br>".join(log_messages) + "</div>", unsafe_allow_html=True)
