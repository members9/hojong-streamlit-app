# âœ… Streamlit ê¸°ë°˜ ìµœì¢… í†µí•© ë²„ì „ (UI + ë¡œì§ í†µí•©)

import streamlit as st
import faiss
import pickle
import numpy as np
import random
import itertools
from collections import deque
from openai import OpenAI

# âœ… ìŠ¤íƒ€ì¼ ë° ë°˜ì‘í˜• CSS ì¶”ê°€
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap');
        html, body, [class*="css"] {
            background-color: #000000 !important;
            color: #FFFFFF !important;
            font-family: 'Noto Sans KR', sans-serif !important;
        }
        .message-box {
            background-color: #F2F2FF;
            color: #000000;
            padding: 12px;
            border-radius: 8px;
            margin-bottom: 20px;
            word-break: break-word;
        }
        .chat-row {
            display: flex;
            flex-direction: column;
            gap: 12px;
        }
        .input-row {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            align-items: flex-end;
            margin-top: 12px;
        }
        .input-row textarea {
            flex-grow: 1;
            min-width: 250px;
            background-color: #1c1c1c;
            color: #fff;
            border-radius: 6px;
            padding: 10px;
            border: 1px solid #444;
        }
        .stButton button {
            background-color: #444;
            color: white;
            padding: 8px 16px;
            border-radius: 6px;
            border: none;
        }
        @media screen and (max-width: 768px) {
            .input-row {
                flex-direction: column;
                align-items: stretch;
            }
        }
    </style>
""", unsafe_allow_html=True)

# âœ… ìƒíƒœ ì´ˆê¸°í™”
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤. ëª¨ë“  ë‹µë³€ì€ ì•„ë˜ ì§€ì¹¨ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:\n\n- ë‹µë³€ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ ë§íˆ¬ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n- ì‚¬ìš©ì ì§ˆë¬¸ì— 'ì¶”ì²œ', 'ì œì‹œ', 'ì°¾ì•„', 'ê²€ìƒ‰í•´' ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ë©´ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ê° í•­ëª©ì— ê¸°ì—…ëª…, ì„œë¹„ìŠ¤ëª…, ê¸°ì—…ID, ì„œë¹„ìŠ¤ìœ í˜•, ê¸ˆì•¡, ê¸°í•œ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.\n- ëª©ë¡ì€ ì•„ë˜ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”:\n  1. \"ì„œë¹„ìŠ¤ëª…\"\n     - \"ê¸°ì—…ëª…\" (ê¸°ì—…ID: XXXX)\n     - ìœ í˜•: ...\n     - ê¸ˆì•¡: ...\n     - ê¸°í•œ: ...\n     - ìš”ì•½: ...\n- íŠ¹ìˆ˜ë¬¸ì(**, ## ë“±)ëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³ , ë¶ˆë¦¿ì€ ëŒ€ì‹œ(-)ë¡œë§Œ í†µì¼í•´ ì£¼ì„¸ìš”. í•­ëª© ê°„ ê°œí–‰ ì—†ì´ ì´ì–´ì„œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n- ê¸°ì—…ëª…/ì„œë¹„ìŠ¤ëª…ì´ ë‚˜ì˜¤ë©´ ë°˜ë“œì‹œ ì´ìœ ë„ í•¨ê»˜ ì„¤ëª…í•´ ì£¼ì„¸ìš”.\n- ê¸°ì—…IDëŠ” ë°˜ë“œì‹œ ê´„í˜¸ ì•ˆì— í‘œê¸°í•´ ì£¼ì„¸ìš”. ì˜ˆ: \"ì œì´ì–´ìŠ¤\" (ê¸°ì—…ID: 12345)"}
    ]
if "all_results" not in st.session_state:
    st.session_state.all_results = deque(maxlen=5)
if "excluded_company_ids" not in st.session_state:
    st.session_state.excluded_company_ids = set()
if "last_results" not in st.session_state:
    st.session_state.last_results = []
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []

# âœ… GPT & FAISS ì¤€ë¹„
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
index = faiss.read_index("service_index.faiss")
with open("service_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

xb = index.reconstruct_n(0, index.ntotal)
xb = xb / np.linalg.norm(xb, axis=1, keepdims=True)
d = xb.shape[1]
index_cosine = faiss.IndexFlatIP(d)
index_cosine.add(xb)

# âœ… í•¨ìˆ˜ ì •ì˜
def get_embedding(text, model="text-embedding-3-small"):
    response = client.embeddings.create(input=[text], model=model)
    return response.data[0].embedding

def ask_gpt(messages):
    response = client.chat.completions.create(model="gpt-4o", messages=messages)
    return response.choices[0].message.content

def is_best_recommendation_query(query):
    return any(k in query for k in ["ì¶”ì²œ", "ê°•ì¶”", "ì œì‹œ", "ì°¾ì•„", "ê²€ìƒ‰í•´"])

def is_relevant_question(query, threshold=0.3):
    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec /= np.linalg.norm(query_vec, axis=1, keepdims=True)
    D, _ = index_cosine.search(query_vec, 1)
    return D[0][0] >= threshold

def recommend_services(query, top_k=5, exclude_company_ids=None):
    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec /= np.linalg.norm(query_vec, axis=1, keepdims=True)
    D, indices = index_cosine.search(query_vec, 100)
    
    candidate_services = {}
    for score, idx in zip(D[0], indices[0]):
        service = metadata[idx]
        cid = service["ê¸°ì—…ID"]
        if exclude_company_ids and cid in exclude_company_ids:
            continue
        key = (cid, service.get("ì„œë¹„ìŠ¤ìœ í˜•"))
        candidate_services.setdefault(key, []).append((score, service))

    best_services = [random.choice(v) for v in candidate_services.values()]
    best_services.sort(key=lambda x: x[0], reverse=True)
    return [s for _, s in best_services[:top_k]]

def make_context(results):
    return "\n".join([
        f"{i+1}. \"{s['ì„œë¹„ìŠ¤ëª…']}\" (ì œê³µê¸°ì—…: \"{s['ê¸°ì—…ëª…']}\", ê¸°ì—…ID: {s['ê¸°ì—…ID']})\n"
        f"- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}\n"
        f"- ê¸ˆì•¡: {s.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')} / ê¸°í•œ: {s.get('ì„œë¹„ìŠ¤ê¸°í•œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ë²•ì¸ì—¬ë¶€: {s.get('ê¸°ì—…ì˜ ë²•ì¸ì—¬ë¶€', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìœ„ì¹˜: {s.get('ê¸°ì—… ìœ„ì¹˜', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•µì‹¬ì—­ëŸ‰: {s.get('ê¸°ì—… í•µì‹¬ì—­ëŸ‰', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- 3ê°œë…„ í‰ê·  ë§¤ì¶œ: {s.get('ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•´ë‹¹ë¶„ì•¼ì—…ë ¥: {s.get('ê¸°ì—… í•´ë‹¹ë¶„ì•¼ì—…ë ¥', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì£¼ìš”ì‚¬ì—…ë‚´ìš©: {s.get('ê¸°ì—… ì£¼ìš”ì‚¬ì—…ë‚´ìš©', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì¸ë ¥í˜„í™©: {s.get('ê¸°ì—… ì¸ë ¥í˜„í™©', 'ì •ë³´ ì—†ìŒ')}"
        for i, s in enumerate(results)
    ])

def make_summary_context(summary_memory):
    seen = set()
    deduped = []
    for result_list in reversed(summary_memory):
        for item in result_list:
            key = (item['ì„œë¹„ìŠ¤ëª…'], item['ê¸°ì—…ëª…'], item.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', ''))
            if key not in seen:
                seen.add(key)
                deduped.insert(0, item)
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•')}\n- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½')}"
        for i, s in enumerate(deduped)
    ])

def make_prompt(query, context, is_best):
    extra = f"ì§€ê¸ˆê¹Œì§€ ì¶”ì²œí•œ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{make_summary_context(st.session_state.all_results)}\n\nì´ì „ì— ì¶”ì²œëœ ê¸°ì—…ë„ í¬í•¨í•´ì„œ ì¡°ê±´ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ìµœê³ ì˜ ì¡°í•©ì„ ì œì‹œí•´ì£¼ì„¸ìš”." if is_best else ""
    return f"""
ë‹¹ì‹ ì€ ê´€ê´‘ìˆ˜í˜œê¸°ì—…ì—ê²Œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” AI ìƒë‹´ì‚¬ í˜¸ì¢…ì´ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{query}"

ê·¸ë¦¬ê³  ê´€ë ¨ëœ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:
{context}

ğŸ“Œ {extra}
"""

# âœ… UI ì¶œë ¥ ì˜ì—­
st.markdown("""
    <h1 style='text-align: center;'>í˜ì‹ ì´ìš©ê¶Œ ì„œë¹„ìŠ¤ íŒŒì¸ë”</h1>
    <p style='text-align: center; font-size:14px;'>ğŸ¤– í˜¸ì¢…ì´ì—ê²Œ ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.</p>
""", unsafe_allow_html=True)

for msg in st.session_state.chat_messages:
    st.markdown(f"<div class='message-box'>{msg['content'].replace(chr(10), '<br>')}</div>", unsafe_allow_html=True)

with st.form("chat_form", clear_on_submit=True):
    st.markdown("<div class='input-row'>", unsafe_allow_html=True)
    user_input = st.text_area("", height=100, label_visibility="collapsed")
    submitted = st.form_submit_button("í˜¸ì¢…ì´ì—ê²Œ ë¬¼ì–´ë³´ê¸°")
    st.markdown("</div>", unsafe_allow_html=True)

if submitted and user_input.strip():
    st.session_state.conversation_history.append({"role": "user", "content": user_input})
    st.session_state.chat_messages.append({"role": "user", "content": user_input})

    if user_input.startswith("ìì„¸íˆ"):
        keyword = user_input.replace("ìì„¸íˆ", "").strip()
        all_results = list(itertools.chain.from_iterable(st.session_state.all_results))
        matches = [s for s in all_results if keyword in s["ê¸°ì—…ëª…"]]
        if not matches:
            reply = "â— í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê¸°ì—…ëª…ì´ ì—†ìŠµë‹ˆë‹¤."
        elif len(matches) > 1:
            reply = "â— ì—¬ëŸ¬ ê°œì˜ ê¸°ì—…ëª…ì´ ì¼ì¹˜í•©ë‹ˆë‹¤:<br>" + "<br>".join(f"- {s['ê¸°ì—…ëª…']}" for s in matches)
        else:
            s = matches[0]
            details = [f"â€¢ {k}: {v}" for k, v in s.items()]
            reply = "<br>".join(details)
        st.session_state.chat_messages.append({"role": "assistant", "content": reply})
        st.rerun()
    else:
        if not is_relevant_question(user_input):
            msg = "â— ê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ ê´€ë ¨ ì§ˆë¬¸ìœ¼ë¡œ ë‹¤ì‹œ ë§ì”€í•´ ì£¼ì„¸ìš”."
            st.session_state.chat_messages.append({"role": "assistant", "content": msg})
            st.rerun()

        best_mode = is_best_recommendation_query(user_input)
        exclude = None if best_mode else st.session_state.excluded_company_ids
        results = recommend_services(user_input, exclude_company_ids=exclude)

        if not best_mode:
            for s in results:
                st.session_state.excluded_company_ids.add(s["ê¸°ì—…ID"])

        st.session_state.last_results = results
        st.session_state.all_results.append(results)

        context = make_context(results)
        prompt = make_prompt(user_input, context, is_best=best_mode)
        st.session_state.conversation_history.append({"role": "user", "content": prompt})

        gpt_reply = ask_gpt(st.session_state.conversation_history)
        st.session_state.conversation_history.append({"role": "assistant", "content": gpt_reply})
        st.session_state.chat_messages.append({"role": "assistant", "content": gpt_reply})

        st.rerun()
