# âœ… ìµœì‹  ë¡œì§ + Streamlit UI/UX í†µí•© ë²„ì „
# CLI í™˜ê²½ì˜ ìµœì‹  ë¡œì§(13_service_recommender.py)ì„ Streamlit UIì— í†µí•©

import streamlit as st
import faiss
import pickle
import numpy as np
import random
from collections import deque
import itertools
from datetime import datetime
from zoneinfo import ZoneInfo  # Python 3.9 ì´ìƒ
import openai
from sentence_transformers import SentenceTransformer

# âœ… ìŠ¤íƒ€ì¼ ë° ë°˜ì‘í˜• CSS ì¶”ê°€
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+KR&display=swap');
        
        html, body, .stApp {
            background-color: #FFFFFF !important;
            color: #0c0c0c !important;
            font-family: 'Noto Sans KR', sans-serif !important;
        }

        /* âœ… ì…ë ¥ì°½ */
        .stTextArea textarea {
            background-color: #f0f0f0 !important;
            color: #0c0c0c !important;
            border-radius: 6px !important;
            padding: 10px !important;
            border: 1px solid #DDDDDD !important;
        }

        /* âœ… ë²„íŠ¼ */
        .stButton > button {
            background-color: #0c0c0c !important;
            color: #FFFFFF !important;
            padding: 10px 16px !important;
            border-radius: 6px !important;
            border: 2px solid #FFFFFF !important;
        }

        /* âœ… ì‚¬ìš©ì/ì±—ë´‡ ë§í’ì„  */
        .user-msg-box {
            text-align: right !important;
        }
        .user-msg {
            display: inline-block !important;
            text-align: left !important; 
            background-color: #FFEB3B !important; 
            color: #0c0c0c !important;
            padding: 10px 14px !important; 
            border-radius: 12px 0px 12px 12px; 
            margin: 0 0 30px 0 !important; 
            max-width: 66% !important;
        }
        .user-msg-time {
            text-align: left !important;
            font-size: 11px;
            color: #666;
            margin-top: 2px;
            width: 100%;
        }    
        .chatbot-msg-box {
            text-align: left !important;
        }
        .chatbot-msg {
            display: inline-block !important; 
            text-align: left !important !important; 
            background-color: #bacee0 !important; 
            color: #0c0c0c !important !important;
            padding: 10px 14px !important; 
            border-radius: 12px 0px 12px 12px !important; 
            margin: 0 0 30px 0 !important; 
            max-width: 66% !important;
        }
        .chatbot-msg-time {
            text-align: right !important; 
            font-size: 11px;
            color: #666;
            margin-top: 2px;
            width: 100%;
        }   
        /* âœ… ê¸°íƒ€ */
        .responsive-title {
            font-size: clamp(40px, 5vw, 60px) !important;
            font-weight: 700 !important;
            color: #0c0c0c !important;  
            text-align: center !important;
            white-space: nowrap !important;
            overflow: hidden !important;
            width: 100% !important;
            padding-bottom: 2px !important;
            margin-top: -52px !important;
        }
        .responsive-subtitle {
            font-size: clamp(14px, 5vw, 12px) !important;
            color: #0c0c0c !important;  
            text-align: center !important;
            white-space: nowrap !important;
            overflow: hidden !important;
            width: 100% !important;
            padding-bottom: 2px !important;
        }
        .user-guide {
            font-size: 14px !important;
            margin-top: 4px !important; 
            color: #0c0c0c !important; 
            text-align: left !important;
            line-height: 1.4 !important;
        }
        
        @media screen and (max-width: 768px) {
            .input-row {
                flex-direction: column !important;
                align-items: stretch !important;
            }
        }
        
        .main .block-container {
            padding-top: 1rem !important;
        }
    </style>
""", unsafe_allow_html=True)

# âœ… ì„¤ì • ë³€ìˆ˜ (13_service_recommender.pyì™€ ì¼ì¹˜í•˜ë„ë¡ ìœ ì§€)
USE_OPENAI_EMBEDDING = True  # ğŸ” ì—¬ê¸°ì„œ ìŠ¤ìœ„ì¹­ ê°€ëŠ¥ (True: OpenAI, False: ë¡œì»¬ ëª¨ë¸)
SIMILARITY_THRESHOLD = 0.30
MAX_HISTORY_LEN = 5  # ì§ˆë¬¸ê³¼ ë‹µë³€ íˆìŠ¤ë¡œë¦¬ ì €ì¥ ì»¨í…ìŠ¤íŠ¸ ê°œìˆ˜

# âœ… OpenAI API í‚¤ ì„¤ì •
openai.api_key = st.secrets["OPENAI_API_KEY"]

# âœ… ë¡œì»¬ ëª¨ë¸ ì´ˆê¸°í™” (í•„ìš” ì‹œ)
if not USE_OPENAI_EMBEDDING:
    local_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# âœ… KST ì‹œê°„ ê°€ì ¸ì˜¤ê¸°
def get_kst_time():
    return datetime.now(ZoneInfo("Asia/Seoul")).strftime("%p %I:%M")

# âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "conversation_history" not in st.session_state:
    st.session_state.conversation_history = deque(maxlen=MAX_HISTORY_LEN + 1)  # system ë©”ì‹œì§€ í¬í•¨ì„ ìœ„í•´ +1
    st.session_state.conversation_history.append({
        "role": "system",
        "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤. "
                "ëª¨ë“  ë‹µë³€ì€ ì•„ë˜ ì§€ì¹¨ì„ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:\n\n"
                "- ë‹µë³€ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ ë§íˆ¬ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
                "- ì‚¬ìš©ì ì§ˆë¬¸ì— \"ì¶”ì²œ\", \"ì œì‹œ\", \"ì°¾ì•„\", \"ê²€ìƒ‰í•´\" ë¼ëŠ” ë‹¨ì–´ê°€ í¬í•¨ë˜ë©´ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ ,"
                " ê° í•­ëª©ì— ê¸°ì—…ëª…, ì„œë¹„ìŠ¤ëª…, ê¸°ì—…ID, ì„œë¹„ìŠ¤ìœ í˜•, ê¸ˆì•¡, ê¸°í•œ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”."
                " ë§Œì¼ ê·¸ë ‡ì§€ ì•Šìœ¼ë©°, ì„œìˆ ì‹ì¼ ê²½ìš° ìì—°ìŠ¤ëŸ½ê³  í¬ê´„ì ì¸ ì„¤ëª…ìœ¼ë¡œ êµ¬ì„±í•´ ì£¼ì„¸ìš”.\n"
                "- ëª©ë¡ìœ¼ë¡œ ì¶œë ¥í•  ê²½ìš° ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:\n"
                "\n"
                "  1. \"ì„œë¹„ìŠ¤ëª…\"\n"
                "     - \"ê¸°ì—…ëª…\" (ê¸°ì—…ID: XXXX)\n"
                "     - ìœ í˜•: ...\n"
                "     - ê¸ˆì•¡: ...\n"
                "     - ê¸°í•œ: ...\n"
                "     - ìš”ì•½: ...\n"
                "     -...\n"
                "\n"
                "- ë°˜ë“œì‹œ ìœ„ í˜•ì‹ì„ ì§€ì¼œì£¼ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ìŠ¤íƒ€ì¼(**, ## ë“±)ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\n"
                "- ë¶ˆë¦¿ì€ í•­ìƒ ëŒ€ì‹œ(-)ë§Œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.\n"
                "- í•­ëª© ê°„ ê°œí–‰ì„ ë„£ì§€ ë§ˆì„¸ìš”.\n"
                "- ê¸°ì—…ëª…/ì„œë¹„ìŠ¤ëª…ì´ ì–¸ê¸‰ë˜ë©´ ë°˜ë“œì‹œ ì´ìœ ë„ í•¨ê»˜ ì œì‹œí•´ ì£¼ì„¸ìš”.\n"
                "- ê¸°ì—…IDëŠ” ë°˜ë“œì‹œ ê´„í˜¸ ì•ˆì— í‘œê¸°í•´ ì£¼ì„¸ìš”. ì˜ˆ: \"ì œì´ì–´ìŠ¤\" (ê¸°ì—…ID: 12345)"
    })
if "all_results" not in st.session_state:
    st.session_state.all_results = deque(maxlen=MAX_HISTORY_LEN)
if "excluded_keys" not in st.session_state:
    st.session_state.excluded_keys = set()
if "last_results" not in st.session_state:
    st.session_state.last_results = []
if "chat_messages" not in st.session_state:
    st.session_state.chat_messages = []
if "embedding_cache" not in st.session_state:
    st.session_state.embedding_cache = {}
if "followup_cache" not in st.session_state:
    st.session_state.followup_cache = {}
if "user_query_history" not in st.session_state:
    st.session_state.user_query_history = []
if "embedding_query_text" not in st.session_state:
    st.session_state.embedding_query_text = None

# âœ… FAISS ì¸ë±ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
@st.cache_resource
def load_index_and_metadata():
    index = faiss.read_index("service_index.faiss")
    with open("service_metadata.pkl", "rb") as f:
        metadata = pickle.load(f)
    
    # ì €ì¥ëœ ì„ë² ë”© ë²¡í„°ë¥¼ ì¬êµ¬ì„±í•˜ê³  ì •ê·œí™”
    xb = index.reconstruct_n(0, index.ntotal)
    xb = normalize(xb)
    d = xb.shape[1]
    index_cosine = faiss.IndexFlatIP(d)
    index_cosine.add(xb)
    
    return index, metadata, index_cosine

index, metadata, index_cosine = load_index_and_metadata()

# âœ… ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def normalize(vecs):
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    return vecs / norms

def get_embedding(text, model="text-embedding-3-small"):
    if text in st.session_state.embedding_cache:
        return st.session_state.embedding_cache[text]

    if USE_OPENAI_EMBEDDING:
        response = openai.Embedding.create(input=[text], model=model)
        embedding = response['data'][0]['embedding']
    else:
        embedding = local_model.encode([text])[0].tolist()

    st.session_state.embedding_cache[text] = embedding
    return embedding

def is_followup_question(prev, current):
    key = (prev.strip(), current.strip())  # ì „ì²˜ë¦¬ëœ ì§ˆë¬¸ ìŒì„ ìºì‹œ í‚¤ë¡œ ì‚¬ìš©

    if key in st.session_state.followup_cache:
        return st.session_state.followup_cache[key]

    messages = [
        {"role": "system", "content": "ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì´ ì´ì „ ì§ˆë¬¸ì— ëŒ€í•œ í›„ì† ì§ˆë¬¸ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨í•´ ì£¼ì„¸ìš”. í›„ì†ì´ë©´ YES, ì•„ë‹ˆë©´ NOë¡œë§Œ ë‹µí•´ ì£¼ì„¸ìš”."},
        {"role": "user", "content": f"ì´ì „ ì§ˆë¬¸: {prev}\ní˜„ì¬ ì§ˆë¬¸: {current}"}
    ]
    try:
        reply = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=messages
        )
        answer = reply['choices'][0]['message']['content'].strip().lower()
        result = "yes" in answer  # 'yes' í¬í•¨ ì—¬ë¶€ë¡œ íŒë‹¨
        st.session_state.followup_cache[key] = result  # âœ… ìºì‹œ ì €ì¥
        return result
    except Exception as e:
        return True  # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ì€ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ê°„ì£¼

def is_best_recommendation_query(query):
    keywords = ["ê°•ë ¥ ì¶”ì²œ", "ê°•ì¶”"]
    return any(k in query for k in keywords)

def is_relevant_question(query, threshold=SIMILARITY_THRESHOLD):
    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec = normalize(query_vec)
    D, _ = index_cosine.search(query_vec, 1)
    max_similarity = D[0][0]
    return max_similarity >= threshold

def is_related_results_enough(ranked_results, threshold=0.35, top_n=3):
    """
    ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ì¶”ì²œ ê²°ê³¼ ì¤‘ ìƒìœ„ Nê°œì˜ í‰ê·  ìœ ì‚¬ë„ê°€ threshold ì´ìƒì¸ì§€ í™•ì¸.
    ê´€ë ¨ë„ê°€ ë‚®ìœ¼ë©´ False ë°˜í™˜ â†’ GPT í˜¸ì¶œ ë°©ì§€ ê°€ëŠ¥.
    """
    if not ranked_results or len(ranked_results) < top_n:
        return False
    top_scores = [score for score, _ in ranked_results[:top_n]]
    avg_score = sum(top_scores) / len(top_scores)
    return avg_score >= threshold

def recommend_services(query, top_k=5, exclude_keys=None, use_random=True):
    # âœ… exclude_keysê°€ Noneì´ë©´ ë¹ˆ ì§‘í•©ìœ¼ë¡œ ì´ˆê¸°í™”
    if exclude_keys is None:
        exclude_keys = set()
    
    # 1. ì§ˆì˜ì— ëŒ€í•œ ì„ë² ë”© ìƒì„± ë° ì •ê·œí™”
    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype('float32').reshape(1, -1)
    query_vec = normalize(query_vec)

    # âœ… 2. ìœ ì‚¬í•œ ì„œë¹„ìŠ¤ 300ê°œ ê²€ìƒ‰ (ê¸°ì¡´ 100ê°œ â†’ í™•ì¥)
    D, indices = index_cosine.search(query_vec, 300)

    # 3. ìœ ì‚¬ë„ ë†’ì€ ìˆœì„œë¡œ (score, service) ëª©ë¡ ìƒì„±
    ranked = [(score, metadata[idx]) for score, idx in zip(D[0], indices[0])]
    # â›” ìœ ì‚¬ë„ ë‚®ì„ ê²½ìš° GPT í˜¸ì¶œë„ ìƒëµí•  ìˆ˜ ìˆë„ë¡ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not is_related_results_enough(ranked):
        return []

    # 4. ì¤‘ë³µ ì œê±° ë° ì œì™¸ ëŒ€ìƒ í•„í„°ë§
    seen_keys = set()
    filtered = []
    for score, service in ranked:
        key = (service["ê¸°ì—…ID"], service.get("ì„œë¹„ìŠ¤ìœ í˜•"), service.get("ì„œë¹„ìŠ¤ëª…"))
        if key in exclude_keys or key in seen_keys:
            continue
        seen_keys.add(key)
        filtered.append((score, service))

    # 5. ìœ ì‚¬ë„ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (ì´ë¯¸ ì •ë ¬ë¼ ìˆìœ¼ë‚˜ ì•ˆì „ ì°¨ì›ì—ì„œ ì¬ì •ë ¬)
    filtered.sort(key=lambda x: x[0], reverse=True)

    # 6. ìƒìœ„ 10ê°œ ì¤‘ ëœë¤ ì„ íƒ or top_kê°œ ì„ íƒ
    if use_random:
        top_10 = filtered[:10]
        selected = random.sample(top_10, min(len(top_10), top_k))
        results = [service for _, service in selected]
    else:
        results = [service for _, service in filtered[:top_k]]

    return results

def ask_gpt(messages):
    response = openai.ChatCompletion.create(model="gpt-4o", messages=messages)
    return response['choices'][0]['message']['content']

def make_context(results):
    """ì¶”ì²œ ê²°ê³¼ ëª©ë¡ì„ ëª©ë¡ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë„ë¡ êµ¬ì„± (ê¸°ì—…ì˜ ìƒì„¸ì •ë³´ í¬í•¨)"""
    return "\n".join([
        f"{i+1}. \"{s['ì„œë¹„ìŠ¤ëª…']}\" (ì œê³µê¸°ì—…: \"{s['ê¸°ì—…ëª…']}\", ê¸°ì—…ID: {s['ê¸°ì—…ID']})\n"
        f"- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}\n"
        f"- ê¸ˆì•¡: {s.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')} / ê¸°í•œ: {s.get('ì„œë¹„ìŠ¤ê¸°í•œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ë²•ì¸ì—¬ë¶€: {s.get('ê¸°ì—…ì˜ ë²•ì¸ì—¬ë¶€', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ìœ„ì¹˜: {s.get('ê¸°ì—… ìœ„ì¹˜', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•µì‹¬ì—­ëŸ‰: {s.get('ê¸°ì—… í•µì‹¬ì—­ëŸ‰', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- 3ê°œë…„ í‰ê·  ë§¤ì¶œ: {s.get('ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- í•´ë‹¹ë¶„ì•¼ì—…ë ¥: {s.get('ê¸°ì—… í•´ë‹¹ë¶„ì•¼ì—…ë ¥', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì£¼ìš”ì‚¬ì—…ë‚´ìš©: {s.get('ê¸°ì—… ì£¼ìš”ì‚¬ì—…ë‚´ìš©', 'ì •ë³´ ì—†ìŒ')}\n"
        f"- ì¸ë ¥í˜„í™©: {s.get('ê¸°ì—… ì¸ë ¥í˜„í™©', 'ì •ë³´ ì—†ìŒ')}"
        for i, s in enumerate(results)
    ])

def make_summary_context(summary_memory):
    seen = set()
    deduplicated = []
    for result_list in reversed(summary_memory):
        for item in result_list:
            key = (item['ì„œë¹„ìŠ¤ëª…'], item['ê¸°ì—…ëª…'], item.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì—†ìŒ'))
            if key not in seen:
                seen.add(key)
                deduplicated.insert(0, item)  # ì›ë˜ ìˆœì„œ ìœ ì§€
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}"
        for i, s in enumerate(deduplicated)
    ])

def make_prompt(query, context, is_best=False):
    if is_best:
        history = make_summary_context(st.session_state.all_results)
        extra = f"ì§€ê¸ˆê¹Œì§€ ì¶”ì²œí•œ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n\n{history}\n\nì´ì „ì— ì¶”ì²œëœ ê¸°ì—…ë„ í¬í•¨í•´ì„œ ì¡°ê±´ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ìµœê³ ì˜ ì¡°í•©ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
    else:
        extra = ""
        
    return f"""ë‹¹ì‹ ì€ ê´€ê´‘ìˆ˜í˜œê¸°ì—…ì—ê²Œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” AI ìƒë‹´ì‚¬ í˜¸ì¢…ì´ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{query}"

ê·¸ë¦¬ê³  ê´€ë ¨ëœ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:
{context}

ğŸ“Œ {extra}
"""

# âœ… UI ì¶œë ¥ ì˜ì—­
st.markdown("""
    <div class="responsive-title">í˜ì‹ ë°”ìš°ì²˜ ì„œë¹„ìŠ¤ íŒŒì¸ë”</div>
    <p class="responsive-subtitle">ğŸ¤– í˜¸ì¢…ì´ì—ê²Œ ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë¬¼ì–´ë³´ì„¸ìš”.</p>
""", unsafe_allow_html=True)

# ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.chat_messages:
    if msg["role"] == "user":
        st.markdown(f"""
        <div class="user-msg-box">
            <div class="user-msg">
                {msg["content"].replace(chr(10), "<br>")}
                <div class="user-msg-time">{msg['timestamp']}</div>
            </div>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="chatbot-msg-box">
            <div class="chatbot-msg"> 
                {msg["content"].replace(chr(10), "<br>")}
                <div class="chatbot-msg-time">{msg['timestamp']}</div>
            </div>
        </div>
        """, unsafe_allow_html=True)

# ì…ë ¥ í¼
with st.form("chat_form", clear_on_submit=True):
    st.markdown("<div class='input-row'>", unsafe_allow_html=True)
    user_input = st.text_area("", height=100, label_visibility="collapsed")
    submitted = st.form_submit_button("ë¬¼ì–´ë³´ê¸°")
    st.markdown("</div>", unsafe_allow_html=True)

# ì‚¬ìš© ì•ˆë‚´ í‘œì‹œ
st.markdown("""
    <div class="user-guide">
        â„¹ï¸ ì‚¬ìš©ë²• ì•ˆë‚´:<br>
        â€¢&nbsp;<b>"ìì„¸íˆ ê¸°ì—…ëª…"</b>ì„ ì…ë ¥í•˜ë©´ í•´ë‹¹ ê¸°ì—…ì˜ ìƒì„¸ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.<br>
        â€¢&nbsp;<b>"ê°•ë ¥ ì¶”ì²œ"</b> ì„ í¬í•¨í•˜ì—¬ ì§ˆë¬¸í•˜ë©´ ì•ì„œ ì œì‹œëœ ë‚´ìš©ë“¤ì„ í¬í•¨í•œ ì „ì²´ ì¶”ì²œì„ ë°›ì•„ë³¼ ìˆ˜ ìˆì–´ìš”.<br>
        â€¢&nbsp;<b>"ì´ˆê¸°í™”"</b>ë¥¼ ì…ë ¥í•˜ë©´ ëŒ€í™” ë‚´ìš©ê³¼ ì¶”ì²œ ê¸°ë¡ì„ ëª¨ë‘ ì§€ìš¸ ìˆ˜ ìˆì–´ìš”.<br>
        â€¢&nbsp;ë³µí•©ì ì¸ ì¡°ê±´ë“¤ì„ ì´ìš©í•œ ì§ˆë¬¸ìœ¼ë¡œ í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•´ ë³´ì„¸ìš”.
    </div>
""", unsafe_allow_html=True)

# ë©”ì‹œì§€ ì²˜ë¦¬ ë¡œì§
if submitted and user_input.strip():
    # ì‹œê°„ëŒ€ ì„¤ì •
    current_time = get_kst_time()
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
    st.session_state.chat_messages.append({"role": "user", "content": user_input, "timestamp": current_time})
    
    # ì´ˆê¸°í™” ëª…ë ¹ ì²˜ë¦¬
    if user_input.lower() == "ì´ˆê¸°í™”":
        st.session_state.embedding_query_text = None
        st.session_state.excluded_keys.clear()
        st.session_state.all_results.clear()
        st.session_state.conversation_history.clear()
        st.session_state.conversation_history.append({
            "role": "system", 
            "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤."
        })
        st.session_state.user_query_history = []
        
        # ì´ˆê¸°í™” ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_messages.append({
            "role": "assistant", 
            "content": "ğŸ¤– í˜¸ì¢…ì´ëŠ” ì ì‹œ ë ˆë“œì¬í•˜ê³  ë‹¤ì‹œ ëŒì•„ì™”ìŠµë‹ˆë‹¤.", 
            "timestamp": current_time
        })
        st.rerun()
    
    # 'ìì„¸íˆ' ëª…ë ¹ ì²˜ë¦¬
    elif user_input.startswith("ìì„¸íˆ"):
        keyword = user_input.replace("ìì„¸íˆ", "").strip()
        all_stored_results = list(itertools.chain.from_iterable(st.session_state.all_results))
        
        if not all_stored_results:
            reply = "â„¹ï¸ ì €ì¥ëœ ì¶”ì²œ ë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤."
        else:
            matches = [s for s in all_stored_results if keyword in s["ê¸°ì—…ëª…"]]
            if not matches:
                reply = "â„¹ï¸ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê¸°ì—…ëª…ì´ ì—†ìŠµë‹ˆë‹¤."
            elif len(matches) > 1:
                reply = "â„¹ï¸ ì—¬ëŸ¬ ê°œì˜ ê¸°ì—…ëª…ì´ ì¼ì¹˜í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”.\n" + "\n".join([f"- {s['ê¸°ì—…ëª…']}" for s in matches])
            else:
                s = matches[0]
                service_link = f"https://www.tourvoucher.or.kr/user/svcManage/svc/BD_selectSvc.do?svcNo={s['ì„œë¹„ìŠ¤ë²ˆí˜¸']}"
                company_link = f"https://www.tourvoucher.or.kr/user/entrprsManage/provdEntrprs/BD_selectProvdEntrprs.do?entrprsId={s['ê¸°ì—…ID']}"
                
                # ìƒì„¸ ì •ë³´ í˜•ì‹í™”
                details = []
                for k, v in s.items():
                    # ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ: ìˆ«ìë¥¼ ì •ìˆ˜, ì½¤ë§ˆ êµ¬ë¶„ í›„ "ì›" ì¶”ê°€
                    if k == "ê¸°ì—… 3ê°œë…„ í‰ê·  ë§¤ì¶œ":
                        try:
                            num = float(v)
                            v = format(round(num), ",") + "ì›"
                        except Exception:
                            pass
                    # ê¸°ì—… ì¸ë ¥í˜„í™©: ì •ìˆ˜ë¡œ í‘œê¸° í›„ "ëª…" ì¶”ê°€
                    elif k == "ê¸°ì—… ì¸ë ¥í˜„í™©":
                        try:
                            num = float(v)
                            v = f"{int(num)}ëª…"
                        except Exception:
                            pass
                    # ê¸°ì—… í•µì‹¬ì—­ëŸ‰: _x000D_ ì œê±°
                    elif k == "ê¸°ì—… í•µì‹¬ì—­ëŸ‰":
                        try:
                            v = v.replace("_x000D_", "")
                        except Exception:
                            pass
                    details.append(f"{k}: {v}")
                
                links = [
                    f"ğŸ”— ì„œë¹„ìŠ¤ ë§í¬: {service_link}",
                    f"ğŸ¢ ê¸°ì—… ë§í¬: {company_link}"
                ]
                reply = "ğŸ“„ ì„œë¹„ìŠ¤ ìƒì„¸ì •ë³´\n" + "\n".join(details) + "\n\n" + "\n".join(links)
        
        # ìƒì„¸ ì •ë³´ ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_messages.append({
            "role": "assistant", 
            "content": reply, 
            "timestamp": current_time
        })
        st.rerun()
    
    # ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬
    else:
        # ëŒ€í™” ì´ë ¥ì— ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        st.session_state.conversation_history.append({"role": "user", "content": user_input})
        
        # ì§ˆë¬¸ ê´€ë ¨ì„± í™•ì¸
        if not is_relevant_question(user_input):
            reply = "â„¹ï¸ ì£„ì†¡í•˜ì§€ë§Œ, ì§ˆë¬¸ì˜ ë‚´ìš©ì„ ì¡°ê¸ˆ ë” ê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ëœ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ í•´ ì£¼ì„¸ìš”."
            st.session_state.chat_messages.append({
                "role": "assistant", 
                "content": reply, 
                "timestamp": current_time
            })
            st.rerun()
        
        # í›„ì† ì§ˆë¬¸ íŒë‹¨
        if st.session_state.user_query_history:
            previous_input = st.session_state.user_query_history[-1]
            if not is_followup_question(previous_input, user_input):
                st.session_state.embedding_query_text = user_input
            # í›„ì† ì§ˆë¬¸ì´ë©´ ì´ì „ ì„ë² ë”© ìœ ì§€
        else:
            # ìµœì´ˆ ì§ˆë¬¸ì¸ ê²½ìš°
            st.session_state.embedding_query_text = user_input
        
        # ì§ˆë¬¸ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        st.session_state.user_query_history.append(user_input)
        
        # ì¶”ì²œ ëª¨ë“œ ì„¤ì • ë° ì„œë¹„ìŠ¤ ì¶”ì²œ
        best_mode = is_best_recommendation_query(user_input)
        exclude = None if best_mode else st.session_state.excluded_keys
        last_results = recommend_services(
            st.session_state.embedding_query_text, 
            exclude_keys=exclude, 
            use_random=not best_mode
        )
        
        # ì¶”ì²œ ê²°ê³¼ê°€ ì—†ì„ ê²½ìš°
        if not last_results:
            reply = "ğŸ§­ ê´€ë ¨ëœ ì¶”ì²œ ê²°ê³¼ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\nê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ëœ ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!"
            st.session_state.chat_messages.append({
                "role": "assistant", 
                "content": reply, 
                "timestamp": current_time
            })
            st.rerun()
        
        # ì¶”ì²œ ê²°ê³¼ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        unique_last_results = [
            s for s in last_results
            if (s["ê¸°ì—…ID"], s.get("ì„œë¹„ìŠ¤ìœ í˜•"), s.get("ì„œë¹„ìŠ¤ëª…")) not in st.session_state.excluded_keys
        ]
        context = make_context(unique_last_results)
        gpt_prompt = make_prompt(user_input, context, is_best=best_mode)
        
        # GPTì— í”„ë¡¬í”„íŠ¸ ì „ë‹¬
        st.session_state.conversation_history.append({"role": "user", "content": gpt_prompt})
        
        try:
            gpt_reply = ask_gpt(list(st.session_state.conversation_history))
        except Exception as e:
            gpt_reply = f"âš ï¸ ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”: {str(e)}"
            
        # ì‘ë‹µì„ ì €ì¥í•˜ê³  ëŒ€í™” ì´ë ¥ì— ì¶”ê°€
        st.session_state.conversation_history.append({"role": "assistant", "content": gpt_reply})
        
        # ì´ë²ˆ ì¶”ì²œì—ì„œ ì–¸ê¸‰ëœ ì„œë¹„ìŠ¤ë“¤ ì œì™¸ ëŒ€ìƒìœ¼ë¡œ ë“±ë¡
        mentioned_keys = {
            (s["ê¸°ì—…ID"], s.get("ì„œë¹„ìŠ¤ìœ í˜•"), s.get("ì„œë¹„ìŠ¤ëª…"))
            for s in last_results
            if (
                str(s["ê¸°ì—…ID"]) in gpt_reply and
                s["ì„œë¹„ìŠ¤ëª…"] in gpt_reply
            )
        }
        
        # ì œì™¸ ëŒ€ìƒ ì—…ë°ì´íŠ¸
        st.session_state.excluded_keys.update(mentioned_keys)
        
        # ì¶”ì²œ ê²°ê³¼ ì €ì¥
        st.session_state.last_results = last_results
        st.session_state.all_results.append(last_results)
        
        # ì±—ë´‡ ì‘ë‹µ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.chat_messages.append({
            "role": "assistant", 
            "content": gpt_reply, 
            "timestamp": current_time
        })
        
        st.rerun()