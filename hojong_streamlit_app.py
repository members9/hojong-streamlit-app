import faiss
import pickle
import numpy as np
import streamlit as st
from openai import OpenAI
from collections import deque

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=st.secrets["OPENAI_API_KEY"])

# FAISS ë° ë©”íƒ€ë°ì´í„° ë¡œë“œ
index = faiss.read_index("service_index.faiss")
with open("service_metadata.pkl", "rb") as f:
    metadata = pickle.load(f)

def normalize(vecs):
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    return vecs / norms

# ë²¡í„° ì •ê·œí™” ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì¸ë±ìŠ¤ êµ¬ì„±
xb = index.reconstruct_n(0, index.ntotal)
xb = normalize(xb)
d = xb.shape[1]
index_cosine = faiss.IndexFlatIP(d)
index_cosine.add(xb)

SIMILARITY_THRESHOLD = 0.30

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []
if "last_results" not in st.session_state:
    st.session_state.last_results = []
if "excluded_company_ids" not in st.session_state:
    st.session_state.excluded_company_ids = set()
if "all_results" not in st.session_state:
    st.session_state.all_results = deque(maxlen=3)
if "user_input" not in st.session_state:
    st.session_state.user_input = ""
if "selected_service" not in st.session_state:
    st.session_state.selected_service = None

def get_embedding(text, model="text-embedding-3-small"):
    response = client.embeddings.create(input=[text], model=model)
    return response.data[0].embedding

def is_best_recommendation_query(query):
    keywords = ["ê°€ì¥", "ìµœê³ ", "ì œì¼", "1ë“±", "1ìœ„", "ì§„ì§œ ì¶”ì²œ", "ê°•ë ¥ ì¶”ì²œ", "ì •ë§ ì¶”ì²œ", "ìµœì„ ì˜ ë°©ì•ˆ"]
    return any(k in query for k in keywords)

def recommend_services(query, top_k=5, exclude_company_ids=None):
    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype("float32").reshape(1, -1)
    query_vec = normalize(query_vec)

    D, indices = index_cosine.search(query_vec, 100)
    results = []
    seen_companies = set(exclude_company_ids) if exclude_company_ids else set()

    if exclude_company_ids is None:
        seen_companies = set()  # ê°€ì¥ ì¶”ì²œì¼ ê²½ìš° ë¬´ì‹œ

    for i in indices[0]:
        service = metadata[i]
        cid = service["ê¸°ì—…ID"]
        if cid not in seen_companies:
            results.append(service)
            seen_companies.add(cid)
        if len(results) == top_k:
            break
    return results

def is_relevant_question(query, threshold=SIMILARITY_THRESHOLD):
    query_vec = get_embedding(query)
    query_vec = np.array(query_vec).astype("float32").reshape(1, -1)
    query_vec = normalize(query_vec)
    D, _ = index_cosine.search(query_vec, 1)
    max_similarity = D[0][0]
    st.session_state.similarity_score = max_similarity
    return max_similarity >= threshold

def ask_gpt(messages):
    response = client.chat.completions.create(model="gpt-4o", messages=messages)
    return response.choices[0].message.content

def make_context(results):
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']})\n- ìœ í˜•: {s.get('ì„œë¹„ìŠ¤ìœ í˜•', 'ì •ë³´ ì—†ìŒ')}\n- ìš”ì•½: {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}\n- ê¸ˆì•¡: {s.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì •ë³´ ì—†ìŒ')} / ê¸°í•œ: {s.get('ì„œë¹„ìŠ¤ê¸°í•œ', 'ì •ë³´ ì—†ìŒ')}"
        for i, s in enumerate(results)
    ])

def make_summary_context(summary_memory):
    seen = set()
    deduplicated = []
    for batch in reversed(summary_memory):
        for item in batch:
            key = (item['ì„œë¹„ìŠ¤ëª…'], item['ê¸°ì—…ëª…'], item.get('ì„œë¹„ìŠ¤ê¸ˆì•¡', 'ì—†ìŒ'))
            if key not in seen:
                seen.add(key)
                deduplicated.insert(0, item)
    return "\n".join([
        f"{i+1}. {s['ì„œë¹„ìŠ¤ëª…']} ({s['ê¸°ì—…ëª…']}) - {s.get('ì„œë¹„ìŠ¤ìš”ì•½', '')}"
        for i, s in enumerate(deduplicated)
    ])

def make_prompt(query, context, is_best=False):
    if is_best:
        history = make_summary_context(st.session_state.all_results)
        extra = f"ì§€ê¸ˆê¹Œì§€ ì¶”ì²œí•œ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n{history}\nì´ì „ì— ì¶”ì²œëœ ê¸°ì—…ë„ í¬í•¨í•´ì„œ ì¡°ê±´ì— ê°€ì¥ ë¶€í•©í•˜ëŠ” ìµœê³ ì˜ ì¡°í•©ì„ ì œì‹œí•´ì£¼ì„¸ìš”."
    else:
        extra = "ì´ì „ ì¶”ì²œëœ ê¸°ì—…ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ìƒˆë¡œìš´ ì¶”ì²œì„ ìµœëŒ€ 5ê°œê¹Œì§€ ë¶€íƒë“œë¦½ë‹ˆë‹¤."
    return f"""ë‹¹ì‹ ì€ ê´€ê´‘ìˆ˜í˜œê¸°ì—…ì—ê²Œ ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•˜ëŠ” AI ìƒë‹´ì‚¬ í˜¸ì¢…ì´ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
"{query}"

ê·¸ë¦¬ê³  ê´€ë ¨ëœ ì„œë¹„ìŠ¤ ëª©ë¡ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤:
{context}

ğŸ“Œ {extra}

ğŸ“Œ ë‹¤ìŒ ì¡°ê±´ì„ ì§€ì¼œì„œ ì¶”ì²œí•´ì£¼ì„¸ìš”:
1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì† ì¡°ê±´ì´ë‚˜ ëª©ì ì„ ìš°ì„  íŒŒì•…í•˜ì„¸ìš”.
2. ë™ì¼í•œ íšŒì‚¬ ë˜ëŠ” ì„œë¹„ìŠ¤ëŠ” ì¤‘ë³µí•˜ì§€ ë§ê³ , ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ ì¤‘ì‹¬ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”.
3. ì¡°ê±´ì„ ì¼ë¶€ ì™„í™”í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ëª©ì ì„ ê°€ì§„ ëŒ€ì²´ ì„œë¹„ìŠ¤ë„ ì¶”ì²œ ê°€ëŠ¥í•©ë‹ˆë‹¤.
4. ê° ì¶”ì²œì€ ë²ˆí˜¸ë¥¼ ë¶™ì´ê³ , ê¸°ì—…ëª…, ì„œë¹„ìŠ¤ëª…, ì„œë¹„ìŠ¤ ìœ í˜•, ê¸ˆì•¡, ê¸°í•œ, ì¥ì , ë‹¨ì , ì¶”ì²œì´ìœ ë¥¼ ë¶„ì„ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
5. ì„œë¹„ìŠ¤ëª…ê³¼ ê¸°ì—…ëª…ì€ ë°˜ë“œì‹œ ë”°ì˜´í‘œ(")ë¡œ ë¬¶ê³ , í•­ëª© í‘œì‹œëŠ” ë°˜ë“œì‹œ ëŒ€ì‹œ(-)ë¡œë§Œ í•´ì£¼ì„¸ìš”.
6. íŠ¹ìˆ˜ë¬¸ì ì—†ì´ ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ ìƒë‹´ì‚¬ ë§íˆ¬ë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
"""

# UI í™”ë©´ êµ¬ì„±
st.title("ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ ì¶”ì²œ AI ğŸ¤–")
st.markdown("ì„œë¹„ìŠ¤ ì¶”ì²œì„ ì›í•˜ì‹œëŠ” ì§ˆë¬¸ì„ í•˜ì‹œë©´, í˜¸ì¢…ì´ê°€ ë„ì™€ë“œë¦½ë‹ˆë‹¤!")

# ìì„¸íˆ ê¸°ì—…ëª… ì²˜ë¦¬
if st.session_state.user_input.strip().startswith("ìì„¸íˆ"):
    keyword = st.session_state.user_input.replace("ìì„¸íˆ", "").strip()
    st.write(f"âœ… í˜„ì¬ keyword: {keyword}")

    all_latest_services = [s for batch in st.session_state.all_results for s in batch]
    st.write("âœ… í˜„ì¬ all_results ê¸°ì—…ëª… ëª©ë¡:")
    for s in all_latest_services:
        st.write(s["ê¸°ì—…ëª…"] + " / ")

    matches = [s for s in all_latest_services if keyword in s["ê¸°ì—…ëª…"]]
    if not matches:
        st.warning("âš ï¸ í•´ë‹¹ í‚¤ì›Œë“œë¥¼ í¬í•¨í•œ ê¸°ì—…ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
    elif len(matches) > 1:
        st.warning("âš ï¸ ì—¬ëŸ¬ ê°œì˜ ê¸°ì—…ëª…ì´ ì¼ì¹˜í•©ë‹ˆë‹¤. ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        for s in matches:
            st.markdown(f"- {s['ê¸°ì—…ëª…']}")
    else:
        s = matches[0]
        service_link = f"https://www.tourvoucher.or.kr/user/svcManage/svc/BD_selectSvc.do?svcNo={s['ì„œë¹„ìŠ¤ë²ˆí˜¸']}"
        company_link = f"https://www.tourvoucher.or.kr/user/entrprsManage/provdEntrprs/BD_selectProvdEntrprs.do?entrprsId={s['ê¸°ì—…ID']}"
        with st.expander(f"ğŸ” [{s['ê¸°ì—…ëª…']}] ì„œë¹„ìŠ¤ ìì„¸íˆ ë³´ê¸°", expanded=True):
            for k, v in s.items():
                st.markdown(f"**{k}**: {v}")
            st.markdown(f"[ğŸ”— ì„œë¹„ìŠ¤ ë§í¬]({service_link})")
            st.markdown(f"[ğŸ¢ ê¸°ì—… ë§í¬]({company_link})")

# ì§ˆë¬¸ì°½
with st.form("input_form", clear_on_submit=True):
    user_input = st.text_area("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", key="user_input", height=80, label_visibility="collapsed")
    submitted = st.form_submit_button("ì§ˆë¬¸í•˜ê¸°", use_container_width=True)

# ìœ ì‚¬ë„ ë©”ì‹œì§€ í‘œì‹œ (ì§ˆë¬¸ ì „ì—ë„)
if "similarity_score" in st.session_state:
    st.info(f"ğŸ” ì§ˆë¬¸ê³¼ ê´€ê´‘ê¸°ì—… ì„œë¹„ìŠ¤ê°„ ìœ ì‚¬ë„: {st.session_state.similarity_score:.4f}")

# ì§ˆë¬¸ ì²˜ë¦¬
if submitted and user_input and not user_input.startswith("ìì„¸íˆ"):
    if not is_relevant_question(user_input):
        st.warning("âš ï¸ ì§ˆë¬¸ì˜ ë‚´ìš©ì„ ì¡°ê¸ˆ ë” ê´€ê´‘ê¸°ì—…ì´ë‚˜ ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ëœ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.")
    else:
        best_mode = is_best_recommendation_query(user_input)
        exclude = None if best_mode else st.session_state.excluded_company_ids

        last_results = recommend_services(user_input, exclude_company_ids=exclude)
        st.session_state.last_results = last_results

        if not best_mode:
            for s in last_results:
                st.session_state.excluded_company_ids.add(s["ê¸°ì—…ID"])

        st.session_state.all_results.append(last_results)
        context = make_context(last_results)
        gpt_prompt = make_prompt(user_input, context, is_best=best_mode)

        chat_history = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ê´€ê´‘ê¸°ì—… ìƒë‹´ ì „ë¬¸ê°€ í˜¸ì¢…ì´ì…ë‹ˆë‹¤."},
            {"role": "user", "content": gpt_prompt}
        ]
        reply = ask_gpt(chat_history)

        st.session_state.chat_history.append((user_input, reply))
        st.rerun()

# ëŒ€í™”ì°½
st.markdown("---")
scroll_container = st.container()
with scroll_container:
    for user_msg, ai_msg in st.session_state.chat_history:
        st.markdown(f"**ğŸ™‹ ì‚¬ìš©ì ì§ˆë¬¸:** {user_msg}")
        st.markdown(ai_msg, unsafe_allow_html=True)
    st.markdown("â„¹ï¸ ê° ì¶”ì²œ ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ìœ¼ë©´ 'ìì„¸íˆ ê¸°ì—…ëª…'ì²˜ëŸ¼ ì…ë ¥í•˜ì„¸ìš”.")
